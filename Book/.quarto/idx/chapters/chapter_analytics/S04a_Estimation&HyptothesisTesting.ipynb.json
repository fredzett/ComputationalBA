{"title":"Testen von Hypothesen","markdown":{"headingText":"Testen von Hypothesen","containsRefs":false,"markdown":"\n\n# Introduction\n\n## Why estimate uncertainty?\n\nIn statistics / data analysis we are commonly estimating **parameters** of the population, i.e. we are estimating numerical characteristics such as the mean or the standard deviation, e.g. \n\n- what percentage of US voters will vote for Joe Biden?\n\n- how satisfied are our customers with our service?\n\n- what is the relation between body weight and exercise time?\n\nUnfortunately, most of the time it is not feasible and/or to costly to determine these parameters directly from the population which is why we commonly deal with (representative) samples. We then estimate the parameter in question from our sample and treat it as an\n**estimator** for the true population mean. \n\n- a sample mean is then an estimator for the population mean \n\n- a sample standard deviation is then an estimator for the popoulation standard deviation\n\n- a sample proportion is then an estimator for the population proportion\n\n\n \n\nWhile it is easy to make such estimate it is crucial to understand the uncertainty and the possible margin of error of this estimate. \n\nThere exist numerous examples (e.g. in journalism) where uncertainty around an estimator is not acknowledged and it is common to only state the point estimate (or summary statistic) instead of the plausible range in which the true parameter may lie. A good example is the news about unemployment figures. Many people don't realize that these numbers commonly are an estimate based on a sample. However, in stead of stating the range the point estimate is always communicated thereby treated as if certain. \n\n**In research we cannot be that slopy and need to understand and determine the uncertainty associated with our estimators.**\n\n## How to estimate uncertainty?\n\nOnce we have calculated a point estimate (e.g. the mean of a sample). How sure can we be that this estimate actually is close to the (true) mean of the population? After all, we don't have data about the enitre population, which is why we are **estimating** a statistic from a sample, instead of **calculating** a parameter from a population. \n\nExample:   \nLet's assume we conduct a randomized controlled trial to test the effectivenss of a new drug. Our trial yields that the average outcome differs between the treamtment and the control groups. \n\nHow do we know that this difference is large enough to state that the groups are actually different, i.e. that the drug is actually effective?\n\n- Could this be due to chance?\n\n- Is the difference large enough that chance cannot explain the difference?\n\n- How large does the difference have to be that chance cannot explain the difference?\n\n\nWe cannot assume that the point estimator actually matches the true parameter which is why we should calculate a range of values and a probability that the true parameter is within this range. \n\nThere are two approaches to achieve this:\n\n1. using classical statistical concepts and probability theory\n\n2. using simulation and intuition\n\nIn the following we will cover both approaches. Note that the first approach is usually covered in introductory statistics courses. \n\n# Estimating uncertainty\n\n## Estimation of uncertainty using probability theory\n\n### Sampling distribution\n\nFrom undergraduate statistics you may recall that a sampling distribution is the distribituion of a random sample based estimate (e.g. the mean or the standard deviation). It is the distribution of estimates when taking $n$ different samples.\n\n**Example**\n\nWe will use (fake) population data on online spending for all family households in Little Town. The data contains:\n\na. amount of online spending\n\nb. indicator if family has children (1 = yes, 0 = no)\n\n\n\nLet's assume we are not in possession of this large data set of the enitre population and instead we only have a sample of $n=50$. We are interested in two estimators:\n\na. mean ($\\bar{x}$) online spending\n\nb. proportion ($p$) of families that has children\n\nBelow both parameters are estimated using a sample\n\nLet's repeat this for $1.000$ times to create a sampling distribution. The histograms below give an approximation of the sampling distributions of both estimators. \n\nObviously, in practice we usually only have one sample (e.g. our sample of $5.000$ households). However, the above procedure illustrates that \n\n- in practice many different samples are possible\n\n- each sample therefore generates a different estimator (as proxy for the true parameter)\n\nLet's investigate the above sampling distributions in more detail. \n\n#### Expected value of $\\bar{X}$\n\nTh sampling distribution of $\\bar{X}$ is the probability distribution of all possible values of the sample mean (in our case online spent). Given we are commonly only in possesssion of one sample we are interested in the mean of all possible values, the **expected value**. \n\nThe **expected value of $\\bar{X}$** is given by:\n\n$$E(\\bar{X}) = \\mu$$\n\n$\\text{where}$  \n$E(\\bar{X}) = \\text{the expected value of $\\bar{X}$}$  \n$\\mu = \\text{the mean of the population from which the sample is selected}$\n\nYou may recall from previous statistical courses that this is called an **unbiased** estimator, as the expected value of a point estimator equals the population parameter.\n\n#### Standard deviation of $\\bar{X}$\n\nIt can be shown that whenever\n\n- the population is infinite or\n\n- the population is finite and the sample size is less than or equal to $5\\%$ of the population size ($\\frac{n}{N} \\leq 0.05)$\n\nthe standard deviation of $\\bar{X}$ is given by:\n\n$$\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}$$\n\nwhere\n\n$\\sigma_{\\bar{x}} = \\text{the standard deviation of $\\bar{X}$}$  \n$\\sigma = \\text{the standard deviation of the population}$  \n$n=\\text{the sample size}$  \n\n\nTo avoid confusion and to make the distinction between standard deviation of the population / sample and the standard deviation of a point estimate we call the latter **standard error**.\n\nFor the remainder of this course we assume that the population size is large and that the above condition holds.\n\n\n\n\n\nThe **standard error** for our example is:\n\nWhich is fairly close to the standard devision of the sampled means\n\n#### Form of the sampling distribution of $\\bar{X}$\n\nLet's consider the following two cases:\n\n1. Population data is normally distributed\n\n2. Population data is not normally distributed\n\n\nIn 1. the sampling distribution will also have a normal distribution.\n\nIn 2. the **central limit theorem** applies, i.e. with large n the sampling distribution for $\\bar{X}$ also can be approximated with a normal distribution.\n\n \n> **Recall: Central limit theorem**  \n> when selecting random sample of size $n$ from a population, the sampling distribution of the sample mean $\\bar{X}$ is approximated by a normal distribution as the sample size becomes large. \n\nLet's have a look at a few examples \n\n#### Why do we care?\n\nWe care about the sample distribution and its properties because it provides useful information about the **difference between the sample mean and the population mean**.\n\n**Example:** let us assume that we take the sample mean online-spent as an approximation of the population mean online-spent. However, obviously we are only accepting this approximation if we it likely that the sampled mean is close to the true mean. \n\nIf we are willing to accept a deviation of $1.000$ from the true mean, what is the probability that the sample mean lies within a range of $1.000$ of the population mean?\n\nWe can answer this question using our sampling distributions of $\\bar{X}$ by calculating a z-score (i.e. standard normalizing our sampled values) and then use the CDF of the standard normal distribution to calculate probabilities:\n\n$$z = \\frac{(x \\pm \\bar{x})}{\\sigma_{\\bar{x}}}$$\n\nWe see that the sample mean - given our sample size - has only a probability of $\\approx 50\\%$ to be within a range of $\\pm 1.000$ from the population mean.\n\nWe can increase this probability by increaing the sample size given the **standard error** is a function of sample size $n$.\n\nLet's calculate standard error and $z$ for a sample size of 300 instead. \n\nIncreasing the sample size to 300 we increasd the probability of the sample mean being within a range of $\\pm 1.000$ of the population mean. \n\n**Note:** analysis of $p$ will be left as an (homework) excercise for you.\n\n### Interval estimation\n\nGiven point estimators cannot be expected to be an exact value of the true population parameter we usually determine **interval estimates**. We do so by substracting and adding **margin of error** from the point estimator. \n\nLet's look into more detail how this is done.\n\n#### Population standard deviation $\\sigma$ is known\n\nAlthough this is usually not the case let's assume that we can reasonably derive knowledge about the population standard deviation ($\\sigma$) from historical data. \n\nHow can we then construct interval estimates?\n\n**Interval estimate of a population mean (when $\\sigma$ is known):**\n\n$$\\bar{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}$$\n\nwhere\n\n$(1-\\alpha) = \\text{the confidence coefficient or level}$\n$z_{\\alpha/2} = \\text{the z value providing an area $\\alpha/2$ in the upper tail of the standard normal probability distribution}$\n\n**Example**: we take a customer survey with $80$ customers to determine customer satisfaction. \n\n(Let's assume that customer satisfaction follows a normal distribution with $\\mu=80$ and $\\sigma = 12$)\n\nFrom our many previous customer surveys we \"know\" (i.e. we assume) that the standard deviation of customer satisfaction is 12. \n\nWe want to be 95% sure that the interval estimate contains the true population mean (i.e. the true customer satisfaction score). This means that $\\alpha = 5\\%$ and we need to determine the $z_{\\alpha/2}$, i.e. the value of the standard normal distribution where only $2.5\\%$ ($\\alpha/2$) are above.\n\nWe are looking for the value $z$ where `stats.norm(0,1).cdf(z)` $\\approx 2.5\\%$. This could be done using trial and error. However, it is easier to use the `stats.norm.ppf` function which does exactly this.\n\nWith this interval we are 95% sure that the interval contains the true population mean. \n\n#### Population standard deviation $\\sigma$ is unknown\n\nIn many real life cases we don't know what the population standard deviation is. If we knew we mostly wouldn't need to calculate estimates in the first place. If $\\sigma$ is unknown we must use the **sample standard deviation ($s$)**. \n\nIn this case the margin of error and the interval estimate is based on the probability distribution known as the **$t$ distribution**.\n\n\nThe t distribution is similar to the normal distribution. A specific t distribution depends on one parameter, the **degree of freedom**. \n\nWith higher degrees of freedom the t-distribution can be approximated by the normal distribtution. Which can be shown when calculating the t and z values repectively. As a rule of thumb we can assume that with a degree of freedom of $\\geq 100$ the standard normal z value provides a good approximation to the t value.\n\nHow do we calculate interval estimates?\n\n**Interval estimate of a population mean (when $\\sigma$ is unknown):**\n\n$$\\bar{x} \\pm t_{\\alpha/2}\\frac{s}{\\sqrt{n}}$$\n\nwhere\n\n$s = \\text{the sample standard deviation}$  \n$(1-\\alpha) = \\text{the confidence coefficient or level}$  \n$t_{\\alpha/2} = \\text{the t value providing an area $\\alpha/2$ in the upper tail of the (student's) t probability distribution}$\n\n**Example**\n\nWe have conducted a study with $70$ University sutdents to estimate the mean credit card debt for a population of University students. Given we don't know anything about the population standard deviation we have to work with our sample.  \n\n- we compute mean ($\\bar{x}$) and standard deviation ($s$) of the sample ($n = 70$)  \n- we then compute the margin of error and the interval estimates to determine a range of values that include the true (population) mean with a certainty of $90\\%$\n\n(Note: the sample data is fake data created using np.random)\n\nWith a probability of $90\\%$ the true population mean of University student's credit card debt lies within the above range. \n\nWe can see that for a rather small sample size of $n = 70$ the difference between using the standard normal distribution and the t-distribution makes a difference. The interval estimate using the standard normal distribution is tighter. \n\nThis difference becomes negigible with a larger data set. Below you can see the difference using a sample size of $n = 500$.\n\n## Bootstrapping\n\nSo far we have been using probabilities, theory and assumptions about distributions of populations and samples to calculate estimation intervals. \n\nIt turns out there is another (for many people more intuitive way) of calculating the above estimation intervals: **bootstrapping**.\n\n**Intuition for bootstrapping algorithm(s):**\n\nIn theory we could see how much our sample estimator deviates from the true population parameter if we were able to draw many different sample from the population. We could then see how the sample statistic fluctuates. \n\nBootstraping is build on the assumption that the sample should be a good approximation for the population (otherwise the sampling should be reconsidered because it is e.g. not representative for the population). \n\nTherefore, it is reasonable to create new samples by drawing from our sample. \n\nIn doing so we can see how our estimate changes with different samples and draw conclusions from this. It turns out that this simple approach works surpringly well in many contexts. \n\n**Algorithm for Nonparametric bootstrap:**\n\nGiven your sample data ${\\{z_i\\}_{i=1}^n}$\n\nFor $b = 1 \\cdots B$:\n\n- resample *with replacement* $n$ observations ${\\{z_i^{b}\\}_{i=1}^n}$\n\n- calculate your estimate ${\\hat{\\theta}_b}$, e.g. the mean, using the resampled set of data\n\nthen ${\\{\\hat{\\theta}_b\\}_{b=1}^B}$ is  an approximation for the sampling distribution for $\\hat{\\theta}$. \n\nGiven the sample distribution we can then calculate the standard error (by calculating the standard deviation of the sample) and hence can calculate the estimate intervals.\n\n### Example 1\n\nLet's look at our (fake) credit debt example again.  Let's say we have a sample of $n=500$ students and we want to estimate the mean credit debt of (unknown student) population. We want to make sure that our interval estimate with a $90\\%$ probabiltiy contains the population mean. \n\n**Traditional approach from previous chapter**\n\n**Bootstrapp approach:**\n\nWe can use this distribution to calculate statistics, e.g. the standard error of our estimate by calculating the standard deviation of estimates. \n\nWe can see that the bootstrap algorithm is an extremely powerfull tool to estimate uncertainties associated with an estimator. \n\n### Example 2\n\n**Determining estimation intervals for proportions**\n\nIn our very first example we have used online spent data and calculated two estimators:\n\n1. mean of online spent\n\n2. proportion of people having children. \n\nWe then looked at 1. to show how to calculate interval estimates; in particular we needed to calculate the standard error for the mean to do this. \n\nIn order to calculate the standard error for the proportion, you may recall from undergrad statistics, we need to apply a different formula (because we are looking to calculate the standard deviation from a binomial distributed variable). \n\nthe standard deviation of $p$ is given by:\n\n$$\\sigma_{p} = \\sqrt{\\frac{(p(1-p)}{n}}$$\n\nwhere\n\n$p = \\text{proportion}$   \n$n = \\text{the sample size}$\n\n\nUsing the bootsrap approch we can calculate interval estimates for the proportion directly without knowledge about the above formular given we approximate the sample distribution and can thus determine the required statistics directly.\n\n### Example 3\n\n**Determine confidence interval for linear regression coefficient**\n\nWe can use the bootstrap algorithm for model parameters such as regression coefficients as well. This shows the flexibility and power of using bootstrap analysis. \n\nWe will use **advertising** data from [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/data.html) for our next example. \n\nFor our example the data contains\n\n- data on TV advertising spent\n\n- sales data\n\nWe run a simle (linear) regression model of the following type:\n\n$$\\hat{y} \\approx \\beta_0 + \\beta_1x$$\n\nwhere:\n\n$\\hat{y} = $ sales data  \n$x = $ tv advertising spent\n\nWe want to understand if there is a relation between money spent on tv advertising and generated sales. If not, we should stop or alter tv advertisment. In order to determine the relation we estimate $\\beta_1$. \n\nWe can easily calculate coefficients and additional output for regression analysis using the below \"helper\" function which uses special statistical packages for these purposes (we will introduce them in next lecture).\n\nWe can see that $\\beta_1$ (denoted $x_1$) is $0.0475$ with a interval estimates of $0.042$ to $0.053$.\n\n**Let's try to recalculate the confidence intervals for $x1$ using a bootstrap approach.**\n\nOur estimator in this case is not the mean or a proportion but $\\beta_1$ which - you may recal from undergraduate statistics - can be calculated as follows:\n\n$$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$$\n\nWe will have to do the following:\n\n1. implement a function that estimates $\\beta_1$\n\n2. bootstrap $beta_1$ repeatedly to generate a distribution of sampled estimates\n\nWe were able to confirm the confidence intervals (although you may not recall how to actually calculate them) by using a bootstrap approach. \n\nWhile for our examples most statistical package have \"out of the box\" functionality, this approach gives us confidence to calculate uncertainty ranges even when using complex models or estimators where no out of the box functionality might be available.\n\n### Conclusion\n\n**Advantage**\n\n- does not make strong assumptions about population or sample distributions\n\n- often theoretical sampling distributions are not normally distributed\n\n- is to understand without knowledge in probability theory / about probability distributions\n\n- no knowledge about formulas for standard errors required \n\n**Disadvantage**\n\n- not feasible for large samples because of computational effort \n\n- computational effort when sometimes a simple formular yields an answer\n\n## Hypothesis testing\n\nPlease refer to Imai (2017), p. 342 to 362 for background information on hypothesis testing. It is generally assumed that you are familiar with hypothesis testing. \n\n### Examples using python\n\nBelow are some examples of how to calculate simple t-tests. We will not cover this in detail as these standard statistics are calculated by statistical packages we will be using when calculating more difficult models. \n:\nTo get an impression how to conduct such tests in python see below\n\n**Two-tailed test (one sample)**\n\nExample: It is generally assumed that stock returns on average have a daily return of $0$. \n\nLet's build our hypothesis:\n\n\\begin{equation} \\label{eq1}\n\\begin{split}\n& H_0: \\mu = 0 \\\\\n& H_1: \\mu \\neq 0 \n\\end{split}\n\\end{equation}\n\nwhere:\n\n$\\mu = $ daily stock return\n\n\nLet's do this in numpy from scratch. Recall that the test statistic is given by:\n\n$$z = \\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}}$$\n\nThis can also be done using `scipy.stats.ttest_..`\n\n**Two-Sample Test**\n\nExample: let's consider a medical trial where we have to groups the trial group (receiving a new medicine) and a control group (receiving a placebo). We now want to understand of the two groups differ regarding a specific parameter. \n\nLet's build our hypothesis:\n\n\\begin{equation} \\label{eq1}\n\\begin{split}\n& H_0: \\mu_0 = \\mu_1 \\\\\n& H_1: \\mu_0 \\neq \\mu_1 \n\\end{split}\n\\end{equation}\n\nwhere:\n\n$\\mu = $ is e.g. the mean of a specific medical parameter (e.g. blood sugar). \n\n\nLet's implement this in python. Let's create two groups with random data both $\\sim$ $N(100,10)$. A two-sample test should yield that both groups do not differ this is $H_0$ cannot be rejected. \n\nWe can see that this is indeed the case given the pvalue of $69\\%$.\n\n### Caution with p-values\n\nIn 2016 an article in *The American Statistician* ([here](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)) criticized the \n\n\"[...] widespread use of “statistical significance” (generally interpreted as “p 0.05”) as a license for making a claim of a scientific finding (or implied truth) leads to considerable distortion of the scientific process.\"\n\nUnfortunately, this practice is still quite common in research. It should be noted that research does not mean \"finding effects with a p-value below 5%\". While there is nothing wrong or false about the concept of p-values it is sometimes misused. \n\nThis is due to several reasons:\n\na. p value tends to decrease with largers samples, i.e. the larger the sample the higher the probability of finding a \"significant effect\" (ceteris paribus)\n\nb. focus is too much on identifying signficiance while confirming the null hypothesis should actually be considered as a finding as well. Instead behaviors termed \"p-hacking\" becomes common ([see here](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.590.4360&rep=rep1&type=pdf))\n\nc. focus on one value is misleading; instead at least ranges should be reported; at best additional tests and indication for confirmation of research hypotheses should be provided. \n\nd. statistical significance is often confused with practical relevance \n\n\n\n**Why p value may be misleading given a large sample size**\n\nStrategy of p value hacking is dangerous and may lead to wrong research findings. \n\nIn the following we will see that larger samples may lead to statistical effects although the practical relevance of theses statistical significant findings are practically irrelevant. \n\nLet's consider the following example (from Daniela Witten): \n\n- we are testing whether a mean of a random varialbe $X \\sim N(0,1)$ is equal to $0$\n\n- we draw a sample and find that the mean ($\\bar{x}$) is slightly different from 0 (e.g. 0.000001)\n\n- Practically we can conclude for (almost all) practical research hypothesis that $H_0$ can be confirmed and that the mean of a sample is $0$. \n\n- However, depending on the size of your sample correctly calculated p-value of your estimator will actually indicate rejection of said hypothesis. \n\nLet's look at an example:\n\nGiven a large sample size we can use the normal distribution and the z-value to test for significance. \n\nRecall that the test statistic is given by:\n\n$$z = \\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}}$$\n\nAs per our assumption \n\n- $s$ will be 1\n\n- $\\mu_0$ will be zero\n\nThis gives us:\n\n$$z = \\bar{x}*\\sqrt{n} = 0.000001 * \\sqrt{n}$$\n\nIf the sample size is big (e.g. $10^6$) - which in the age of big data cannot be considered an exception anymore - the z value will become very big (yielding a low p-value) and leads to the conclusion that $H_0$ needs to be rejected. \n\nHowever, for all practical reasons $\\bar{x}$ is very close to zero and we only observe this finding given the large sample size. \n\nThis does not mean that p-values are wrong. Statistically speaking the result is correct. However, it should be noted that with large sample size we are to see statistical effects that may in fact no indicate practical findings given the deviance from $H_0$ is negligible.  \n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"S04a_Estimation&HyptothesisTesting.html"},"language":{},"metadata":{"lang":"de","fig-responsive":true,"quarto-version":"1.2.269","theme":"cosmo","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}}}