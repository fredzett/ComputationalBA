{"title":"Tidy Data","markdown":{"headingText":"Tidy Data","containsRefs":false,"markdown":"\n\n# Introduction\n\nSo far we have been mostly using the following modules:\n\n- `scipy`: for probabilties and simulations\n- `numpy`: for numerical operations\n- `matplotlib`: for plotting (note: partly disguised by helper functions provided for this class (e.g. `plot_line`) which abstract away use of matplotlib)\n\nWe have only recently introduced `pandas` as a new module to work with dataframes. When referring to dataframes we mean data objects similar to an **excel sheet** where we would typically have:\n\n- header describing the column data\n- each column data could be of different types (numeric, qualtiative, text etc.)\n- each row is labelled\n\n**Dataframes helpful for reserach**  \nWorking with dataframes is helpful in quantitative research as our data typically doesn't fully consist of numerical data and we also want to give our variables explanatory names. \n\n**Organization of data is important**  \nQuantitative research involves many steps. The data analysis part involves e.g.: \n\n- generating data: e.g. via survey, web scraping, gathering from different sources etc.\n\n- preprocessing data: i.e. preparing the data for analysis; e.g. cleaning the data, filtering data, rearranging data, combining data sets etc.\n\n- exploring data: plotting raw data, calculating descriptive statistics etc.\n\n- analyzing data: e.g. running a regression analysis\n\n- presenting data: e.g. plotting data or creating summary tables\n\nMost university courses focus on the generation and analysis part of the data analysis part. Up unitl now this was also our focus (and will continue to be). However, in practical research one of the most **time consuming** steps is the **preprocessing** step. Some [authors](https://www.wiley.com/en-us/Exploratory+Data+Mining+and+Data+Cleaning-p-9780471268512) claim that this part of the data analysis pipeline takes up about $80$% of the work. \n\n\nTo this end, the this lecture has two purposes:\n\n1. explain how to work with `pandas`\n\n2. explain how to set up a dataframe best suited for quantitative analysis and statistics\n\nBoth shall help you to be able to prepare data for your research. \n\n# Pandas\n\nBefore we want to understand how to best set up a dataframe to serve your research / analysis purposes, let's have a look at the `pandas` module in detail. \n\n## Creating pandas dataframes\n\nWe can create pandas dataframes in two basic fashions:\n\n1. from scratch using other datatypes\n\n2. from reading in a file \n\n**Creating dataframe from scratch using numpy**\n\nWe can also give both index and columns specific names\n\n**Creating dataframe from scratch using dictionaries**\n\n**Index and columns names**\n\nWe can access both index and columns using\n\n**Creating dataframe from files**\n\nIn research it is common that we have data and need to load it into a dataframe. This can be easily done using the \n\n- `pd.read_` commmand\n\nwhich gives you a range of file types that you can read from\n\n![pd.read](https://www.dropbox.com/s/4zx16ag09vcjt6t/pdread.png?dl=1)\n\nLet's look at an example and read in the file `Auto.csv` which is in the folder `data` which is located in the same folder as this notebook.\n\nAll `pd.read_` functions provide a wide variety of loading options. \n\nFor example I could read in the same file with only a specific set of columns (columns 2 and 6). \n\n## Data selection\n\nThere are several ways how to work with data stored in dataframes. Note that one way is to convert the data into numpy arrays and work with them the usual way. This has the advantage of working with data objects we already are familar with. It has the disadvantage that we loose some information (such as header) and functionality (e.g. it is much harder to work with non-numerical data). \n\n\n\n**Selecting columns**\n\nLet's continue and work with the dataframes directly\n\n**Selecting columns using `loc` and `iloc`**\n\nCommonly we want to select subsets of a dataframe (i.e. we want to select rows and columns). This can be done via \n\n- `iloc`: selecting as it we were working with numpy array (i.e. use numerical indexing)\n- `loc`: selecting using explicit index and/or column names\n\nUsing `iloc` indexer\n\nUsing `loc` indexer\n\n**Using more sophisticated indexing**\n\n## Operations on dataframes\n\nPandas is designed to work with numpy which means that most numpy functions work on pandas\n\nYou can also conduct regular operations on dataframes. Example:\n\nColumns and row wise operations\n\nAdd row 8 from all other rows\n\nSubstract column \"b\" from all columns\n\n## Aggregating data\n\nWe can also - as with numpy arrays - aggregate data using pandas.\n\n## Handling missing data\n\nIn real life you will have to handle missing data (in python called `nan`, i.e. not a number). It is important to be able to deal with missing data, e.g.\n\n- filter out all observation where certain variable data is missing (e.g. in a survey people didn't answer certain questions)\n\n- specify how to deal with missing data when calculating descriptive statistics (e.g. the mean)\n\nLet's look at how to deal with missing data\n\n**Determine nans**\n\n**Filtering out nans**\n\n**Filling nan values**\n\nIn practice you don't always have to drop missing data points but could instead impute values for these missing data points. \n\nFor example in stock market data you may have missing data for certain days (out of many days) because the data provider was experiencing a technical issue. In that specific case it may be perfectly reasonable to fill the missing data using some sort of data imputation technique. \n\n## Grouping data\n\nWhen doing quantitative analysis and statistics we are frequently applying calculations on filter and aggregations on data. \n\nLet's look at a simple example dataframe: in the below dataframe we may be interested in the sum of data per key.\n\nThis could be calculated easily with what we have seen before. We could simply apply three filters\n\nThis is tedious (although we could have made the code less repetitive and verbose using a for-loop) and sometimes not practical.\n\nA very powerful feature of pandas is the **groupby** operation / concept which we will introduce in the following\n\nThe way this function works is illustrated in the following figure\n\n![groupby](https://www.dropbox.com/s/rpg9grsod2dhrh1/pdgroupby.png?dl=1)\n\n(note: the example is taken from [Jake VanderPlas' book](https://github.com/jakevdp/PythonDataScienceHandbook)\n\nThe \"split-apply-combine\" mechanic can be very helpful when grouping and aggregating data. \n\nLet's take our `car dataset` and apply some more methods. For example let's say we want to understand if \n\n- mean `mpg` differs \n- when conditioned on \n- origin and number of cylinders\n\nWe can use the *as_index=False* statement to return a dataframe\n\nWe can use the groupby method to quickly explore our dataset when combining with other methods. Most importantly\n\n- aggregate\n- transform\n\n\n**Groupby and aggregate**\n\n**Groupby and transform**\n\n## Plotting data\n\nPandas dataframes can be plotted quite easily\n\n## Pivot tables\n\nPivot tables work similar to excel pivot tables and are helpful for more complex conditional filtering\n\nLet's say we want to calculate average weight conditioned by number of cylinders and origin\n\n## Complex filtering\n\nSometimes we want to apply multiple filter. Here `query` can be handy.\n\n# Tidy data\n\nPlease refer to the great article by Hadley Wickham [Tidy Data](https://vita.had.co.nz/papers/tidy-data.pdf) for a detailed discussion on tidy data.\n\nAccording to Wickham, **tidy data** is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. \n\nIn tidy data:\n\n1. each variable forms a column\n\n2. each observation forms a unit\n\n3. each type of observational unit forms a table \n\nWhy is this important: because when a dataframe is not tidy it is much harder to analyse and explore the data. \n\n**Example** (from Wickham)\n\nThere are different ways how to structure data\n\nOption 1\n\nOption 2\n\nOption 3\n\nWhen doing quantitative or statistical analysis you would typically want to conduct analysis on the two variables \"treatment\" and \"treatment_result\" which is why the third option is much more helpful when conducting analysis and is considered to be **tidy**:\n\n- each variable forms a column: we have three variables name, treatment and treatment result\n- each observation forms a row: we have 6 observations for 3 persons two treatments\n- (number 3: not relevant)\n\nThe following examples are taken from [Daniel Chen](https://github.com/chendaniely/pydatadc_2018-tidy/tree/master/data)\n\n**Example using a bigger data set**\n\nThe table may be useful for presentation purposes and may be data efficient. But it is not tidy.\n\nWhy is it not tidy?\n\nA good proxy to find out if the data is stored in a tidy format is to think about a model you want to build (e.g. a regression model) and aks yourself what possible variables for your models would be. \n\nIn the above case it is unlikely that we would want to treat \"<$10k\" as a variable. Instead we would likely want to use \"income type\" as a variable. So the data set actually consists of three variables:\n\n- religion\n- income\n- count (number of people)\n\nWith the data structure above we are not able to analyze the data effiently. \n\nFor example: the following analysis would already be quite tedious:\n\n- calculate sum of counts per religion\n\n- run a model on different variables\n\nThe format the above table is also called **wide format**.\n\nIn order to make it tidy we need to put it into a **long format** using `pd.melt`\n\nWe can now already answer the first question really easy\n\nAlthough this would also be achievable using the wide format it is (i) much verbose and (ii) also would need change of code if a new income category was introduced\n\n**More complex example**\n\nLet's look at case and death number from an ebola pandemic in 2014/2015. \n\nThe data is not really well structured for analysis given cases and death for each country is organized as columns. However, typically we would ask questions such as:\n\n- what is number of death per country\n- what is average case increase per country \n- etc.\n\nClearly the data is not tidy as we have the following variables in the data set:\n\n- date\n- day\n- indicator (case or death)\n- country\n\nIt would be very difficult to calculate simple statistics such as aggregate deaths by country.\n\nLet's make the dataframe tidy to be able to do these kind of calculations.\n\nWe are almost there. We only need to look more carefully at column \"variable\".\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"S06a_TidyData.html"},"language":{},"metadata":{"lang":"de","fig-responsive":true,"quarto-version":"1.2.269","theme":"cosmo","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}}}