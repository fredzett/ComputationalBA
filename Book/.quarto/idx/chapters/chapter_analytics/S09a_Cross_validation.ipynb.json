{"title":"Cross Validation","markdown":{"headingText":"Cross Validation","containsRefs":false,"markdown":"\n\n### Helper functions\n\n# Cross validation\n\nLet's illustrate the approach using a familiar data set: the `cars` data set. \n\nWe consider building a model to predict `mpg` (miles per gallon) from `horsepower`.\n\nLooking at the data we see that the relationship between `mpg` and `horsepower` is not linear. In Chapter 5 we have learned that we may deal with this non-linearity by transforming one or both variables using logarithms. \n\nAnother strategy is to introduce polynomials into our model, i.e. instead of\n\n$$f(x) = \\beta_0 + \\beta_1X $$\n\nwe could add a term and calculate\n\n$$f(x) = \\beta_0 + \\beta_1X + \\beta_2X^2$$\n\nwhere $X$ is `horsepower`.\n\nLet's quickly calculate both models and compare how good they are before we proceed:\n\n__Considerations__: we can see that the model not only using a linear term but also a squared term is clearly better.\n\nIn a predictive modelling setting we have two challenges now:\n\n1. how do we know that the second model is actually _general_ better (i.e. not only due to the specific observations in the data set)?\n\n2. how do we know if adding even higher-order polynomials does increase the model performance (also taking into consideration the first issue)?\n\n\nWe can answer both questions using __cross validation__, i.e. a technique of holding out a subset to “generate“ a test data set from training data set is also called cross validation\n\nCross validation serves two purposes  \n- model assessment, i.e. evaluate model‘s performance\n- model selection, i.e. selecting a suited model flexibility\n\n\nCross-validation (CV) approaches can be distinguised by way of subsetting the training data set\n\n- Validation set approach\n- Leave-One-Out cross-validation\n- K-Fold cross-validation\n\n\n\n__Example__\n\n\n\nLet's consider to run 7 different models for each model increasing the degree of polynomial. So the models are\n\n\\begin{equation}\n\\begin{split}\nmpg & = \\beta_0 + \\beta_1horsepower \\\\\nmpg & = \\beta_0 + \\beta_1horsepower + \\beta_2horsepower^2 \\\\\nmpg & = \\ldots \\\\\nmpg & = \\beta_0 + \\beta_1horsepower + \\ldots + \\beta_7horsepower^7 \\\\\n\\end{split}\n\\end{equation}\n\n    \nWe will compare test errors of all seven models for th three CV approaches\n\n# Comparing three cross validation approaches\n\n## Validation approach\n\nWe randomly split the data set into two sets both containing half of the data, i.e. given we have a data set with 392 observations\n\n- training data set consists of $\\frac{392}{2} = 196$ observations\n\n- validation data set consists of 196 observations\n\n__Problem:__ results are highly dependent on the random splitting. Each time we repeat the split we get a different train/validation split (i.e. different observations in both splits) and results change. \n\n## Leave one out cross validation\n\nOne disadvantage of the _validation approach_ is that we reduce training size significantly leading to bias in our test error. In addition we see high variance in our test error as it highly depends on the random splitting. \n\n__Leave one out cross validaiton (LOOCV)__ compensates for these issues by following these steps:\n\n- split data in training set consisting of $n-1$ observations and validation set containing $1$ observation\n- fit / calculate model using the training data\n- evaluate model by calculating test error on validation set\n- repeat this step for all $n$ observations ensuring that each observation is in validation set once\n- calculate test error as mean of $n$ test errors\n\n$$CV_{(n)} = \\frac{1}{n}\\sum_{i=1}^{n} MSE_i$$\n\nThe result is not depending on randomness as we used all $n$s for validating. However, the cross validation is compuational expensive for large data sets (as $n$ increases). Even for this small dataset we had to calculate $392*7 = 2744$ models. \n\n## K-fold cross validation\n\nK-fold cross validation mitigates the computational expensiveness of LOOVC by not using $n$ validation sets but only k. K is usually set to $5$ or $10$\n\nWe do the following steps:\n\n- split data into k groups of approximately equal size\n- fit / calculate model using the training data consisting of $k - 1$ groups\n- evaluate model by calculating test error on validation set consisting of the remaining 1 group\n- repeat this step for all $k$ groups ensuring that validation set consists of one group once\n- calculate test error as mean of $k$ test errors\n\n\n$$CV_{(k)} = \\frac{1}{k}\\sum_{i=1}^{k} MSE_k$$\n\nFor $k=5$\n\n## Comparing results for all three approaches\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"S09a_Cross_validation.html"},"language":{},"metadata":{"lang":"de","fig-responsive":true,"quarto-version":"1.2.269","theme":"cosmo","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}}}