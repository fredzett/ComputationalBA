{"title":"Klassifikation","markdown":{"headingText":"Klassifikation","containsRefs":false,"markdown":"\n\n# Logistic regression\n\n## Introduction\n\nMany practical and research problems deal with issues where the _dependent variable_ ($Y$) is _categorical_. Consider, for example, the following problems:\n\n1. will patient be cured by taking a certain medicine or not\n\n2. what are factors driving a customer's decision to cancel his/her contract\n\n3. which of the five available parties will people vote for?\n\nAll of these questions have in common that the dependent variable is categorical. Problems 1 and 2 deal with questions where two alternatives exist, whereas in problem 3 more than two alternatives exist. \n\nNote that even if the dependent variable is not binary - e.g. net income - you may be that your research question or decision relevant information is binary (did you make a loss or a profit). \n\n__Logistic regression__ is well suited to answer questions such as the above. Here the dependent variable $Y$ is a categorical variable whose values ($g = 1, \\ldots, G)$ represent the alternatives available. For example, in problem 2, $Y$ could take the value $0$ or $1$ representing a customer cancelling a contract or not. In problem 3, $Y$ could take values of $0, \\ldots, 4$ to represent the five available parties. Theoretically, other values are possible but commonly we use $0$ and $1$ for the binary case. \n\nWe use different terminology to distinguish between the two cases described above:\n\n1. Binary logistic regression for $G = 2$\n\n2. Multinomial logistic regression for $G \\geq 3$\n\n\n\nThe __logistic regression model__ can be generally expressed as follows:\n\n$$\\pi(x) = f(x_1, \\ldots, x_J)$$\n\nWhere $\\pi(x)=P(Y = 1 | x)$ is the conditional probability that for event 1 (e.g. customer cancels contract) occurs given the independent / explanatory variables $x_1, \\ldots, x_J$. Identical to the linear regression model the explanatory variables are expressed using a linear combination \n\n$$z(x) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_Jx_J$$\n\nThe logistic regression receives its name from the (standard) __logistic function__ \n\n$$p = \\frac{e^z}{1+e^z}= \\frac{1}{1+e^{-z}}$$\n\n(Note: simpliefied by multiplying with $1/e^z$)\n\nPlotting the function (here: bewteen -5 and 5) reveals its s-shaped distribution. The function can hence be interpreted as a cumulative probabilitby distribution function and it is similar to the cumulative (standard) normal distribution as you can see from the below plot:\n\nThe logistic function is applied in a broad range of applications as it is mathematically less complex than the normal distribution. By using a lostistic function we can **transform** real numbers into probabilities, i.e. transform numbers in the range of $[-\\infty, +\\infty]$ to the range $[0, 1]$. Specifically, transforming the _systematic component_ using the _logistic function_ yields the __logistic regression function__:\n\n$$\\pi(x)=\\frac{1}{1+e^{-z(x)}}$$\n\nThe larger the systematic component $z(x)$, the higher $\\pi(x)$. Therefore: the higher $z(x)$, the higher $P(Y = 1 | x)$.\n\nIn line with our discussion in Chapter 5 (linear regresssion) we will now examine how to use logistic regression by explaining\n\n1. model formulation\n\n2. estimation of logistic regression function\n\n3. interpretation of regression coefficients\n\n4. assessment of overall model\n\n5. assessment of coefficients\n\nWe will largely rely on the explanaitons and the dataset used in  [*Backhaus et al (2017)*](https://www.springer.com/de/book/9783662460764).\n\n### Dataset\n**Data set**: the data set contains (toy) data on a product test where people where shown a product (premium butter) and asked if they would buy it. The dataset includes the following variables:\n\n- person: id of person\n\n- income: income of person buying / not buying the product (here: premium butter)\n\n- gender: gender of person buying / not buying the product ($0 =$ female, $1 =$ male)\n\n- purchase: indicator for purchasing decision ($0=$ no purchase, $1=$ purchase)\n\nThe data has 30 observations.\n\n## Model formulation\n\nLet's consider a simple example where we analyze the purchase decision and treat only _income_ as a explanatory variable. This yields the following model:\n\n$$Y = f(\\text{income})$$\n\nwhere $Y=1$ for purchase and $0$ for no purchase. \n\nWe are interested in how income is driving the purchase decision. Let's look at this relationship graphically first:\n\nThe upper marks represent people who stated that the would purchase the product. The lower marks represents people who indicated that they would not purchase the product. From the plot it becomes obvious that there is no exact relationship between income and purchase. For example, for the income of $\\sim 2.500$ there is a mark for both purchase and no purchase. \n\nHowever, visually it appears that the higher the income the more likely is the purchasing decision. For example, above an income of $\\sim2.600$ there are only purchasing decisions.\n\nLet us try to formulate a model that captures this realationship.\n\n### Linear (probability) model\n\nThe simplest model is the linear model which is given by:\n\n$$\\pi(x_k) = \\beta_0 + \\beta_1x_k$$\n\nHere $\\pi(x_k)$ gives the purchase probabilities of test persons. These probabilities are modelled such that they are linear dependent on the income of the test persons. While the probabilities cannot be observed they are derived from the purchasing data. \n\nLet's calculate the model using python:\n\n  \n\nThe model is already known and quite simple. Especially we already know how to interpret the model. For example, given the mean income is $2.000$ we can say that the purchase probability at the this income level is \n\n$$p(purchase) = -0.2833 + 0.3861 \\times 2 \\approx 0.5$$\n\nTherefore the probability is approx. 50% that a person with an income of $2.000$ would purchase the product. This is easy to interpret and we are alredy familiar with all the model characteristics from chapter 5. However, the model is logically flawed given the probabilities can take values of below 0 and above 1 in theory. For our specific example a person with an income of $5.000$ would have a (calculated) probability of \n\n$$p(purchase) = -0.2833 + 0.3861 \\times 5 \\approx 1.65,$$\n\ni.e. there would be a probability of 165% calculated by the model. \n\n\n### Logistic regression\n\nThe binary logistic regression model is expressed as follows:\n\n$$\\pi(x_k) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_k)}}$$\n\nThe model assumes that $Y_k$ (i.e. the purchase decisions) is a binary, independent random variable with expected value $E(Y_k | x_k) = \\pi(x_k)$. To this end, the model assumes that $Y_k$ is _bernoulli_ distributed. \n\n\nLet's calculate the logistic regression model using the `statsmodels` package: this can easily be done using a similar syntax as for the linear regression model:\n\n`sm.Logit(y,X)` instead of `sm.OLS(y,X)`\n\n   \nThe parameter of the model are \n\n- $\\beta_0 = -3.6707$\n- $\\beta_1 = 1.8272$\n\nwhich yields the following logistic function:\n\n$$p(x_k) = \\frac{1}{1+e^{-(-3.6707 + 1.8272x_k)}}$$\n\nIncluding the logistic function in our plot we see that the slope of the function is quite similar to the linear model. However, the function is s-shaped and can only take values between $0$ and $1$ by definition which makes it better suited for our analysis. \n\n#### Classification\n\nHow can we use the estimated probabilities for prediction of purchase behavior? \n\nIn order to do so we need to define a __cut off value__ $p*$ such that\n\n\\begin{equation}\n  y_k =\\begin{cases}\n    1, & \\text{if $p_k > p*$} \\\\\n    0, & \\text{if $p_k \\leq p*$}\n  \\end{cases}\n\\end{equation}\n\nCommonly, a cut of value of $0.5$ is used. This means that \n\n- all $y_k$ with a value (probability) of above 50% are assigned 1 and\n- all $y_k$ with a value (probability) of equal or below 50% are assigned a 0\n\nLet's analyse how good our logistic regression model is in classifying purchasing decisions using our data. \n\nWe can use our estimated model and predict _purchase probability_ based on _income_ data. We can then compare with our true values to see how could our model actually is. \n\n#### Confusion matrix\n\nIf we want to understand how good our prediction was we can calculate the __confusion matrix__. It calculates true values vs. predicted values for each category. A summary of this overview is helpful as it gives inside to three different measures:\n\n- accuracy: % of correct predictions \n- specificity: % of correctly classified non purchases in relation to all non-purchases (correct false)\n- sensitivity: % of correctly classified purchases in relation to all purchases (correct trues)\n\nLet's look at the below table summarizing the results:\n\n|       \t| Prediction \t|    \t|       \t|           \t|             \t|\n|-------\t|------------\t|----\t|-------\t|-----------\t|-------------\t|\n|       \t| 0          \t| 1  \t| Total \t| % correct \t|             \t|\n| 0     \t| 7          \t| 7  \t| 14    \t| 50,00%    \t| Specificity \t|\n| 1     \t| 7          \t| 9  \t| 16    \t| 56,25%    \t| Sensitivity \t|\n| Total \t| 14         \t| 16 \t| 30    \t| 53,33%    \t|   Accuracy  \t|\n\n\naccuracy: \n- we have classified 7 non-purchases and 9 purchases correcty\n- in total there were 30 observations\n- (7+9)/30 = 0.533\n\n__sensitivity__, i.e. proportion of correctly predicted purchases in relation to all purchases\n- we have classified 9 purchases out of 16 purchases correctly\n- 9 / 16 = 0.563\n\n__specificity__, i.e. proportion of correctly predicted non-purchases in relation to all non-purchases\n- we have classified 7 non-purchases out of 14 non-purchases correctly\n- 7 / 14 = 0.500\n\n\nIn more general form we can say:\n\n|   \t| Prediction     \t|                \t|\n|---\t|----------------\t|----------------\t|\n|   \t| 0              \t| 1              \t|\n| 0 \t| True negative  \t| False positive \t|\n| 1 \t| False negative \t| True positive  \t|\n\n\n$$\\text{accuracy} = \\frac{TN + TP}{\\text{total observations}}$$\n\n$$\\text{sensitivity} = \\frac{TP}{\\text{TP + FN}}$$\n\n$$\\text{specificity} = \\frac{TN}{\\text{TN + FP}}$$\n\nWhere:\n- false positive is also called the Type I error and\n- false negative is also called the Type II error\n\nIt is important to look at all three measures for several reasons, e.g. accuracy may be misleading for unbalanced datasets  (e.g. fraudulent credit card transactions).\n\nAlso, it is important to be aware of the fact that there is a tradeoff between specificity and sensitivity. For example we can easily build a model that has a sensitivity of 100%. We could do so by simply classifying all observations as purchases. This would give a 100% sensitivity. On the other hand the specificity would be at 0% as we have not correctly classified one non-puchase observations. \n\n**Example sensitivity and specificity: diagnosis of a disease**\n\n- sensitivity: \"patient is sick and test is positive\"\n\n- specificity: \"patient is not sick and test is negative\"\n\nWhen is high sensitivity or high specificity good or bad?   \n- disease can be cured if diagnosed early: than a high sensitivity is good\n- disease cannot be cured: than a high specificity is good as a high sensitivity increases chance for false positive and a wrongful diagnosis would leave a healthy person with the diagnosis that he/she has a uncurable disease.\n\n\nWe can eassily calculate the measures manually.\n\n#### ROC Curve\n\nOne disadvantage of the confusion matrix is that it is a snapshot for one specific cutoff value $p*$. The __ROC curve__ visualizes correctness of classification over the continuum of possible cutoff points. \n\nThe ROC curve plots sensitivity vs. 1 - specificity for different cutoff points. The diagonal line represents a model that a random prediction. A line along the top left corner represents a good classifier given it is able to \n\n- correctly identify purchases from all purchases while\n- not wrongly identifying many purchases (e.g. if sensitivity is 1  because we simply always predict 1, then all non-purchases are wrongly predicted)\n\n### Multiple logistic regression\n\nUp until now and similar to the linear regression model we have build a very simple model explaining the decision to purchase with exactly one (explanatory) variable: income. Intuitively there may be more than one factor / one variable driving the purchasing decision. Similar to the regression example we can extend the (binary) model to the case where multiple variables are included. This is called the __multiple logistic regression__.\n\nLet $x_k = (x_{1k}, x_{2k}, \\ldots, x_{Jk})$ be the set of values / observations for $J$ independent variables. Then the multiple logistic regression model is defined as (neglecting index $k$):\n\n$$\\pi(x) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_Jx_J}}$$\n\nThis is commonly expressed in matrix form as\n\n$$\\pi(\\mathbf{x}) = \\frac{1}{1+e^{-\\mathbf{x}\\mathbf{\\beta'}}}$$. \n\nFor our example data the model could include _gender_ as a second explanatory variable. This would give us the following model:\n\n$$Y = f(\\text{income, gender})$$\n\nwhich would lead to the following logistic model:\n\n$$\\pi(x_k) v\n\nThe logistic regression function for our model therefore looks as follows:\n\n$$p(\\mathbf{X}_k) = \\dfrac{1}{1+e^{-(-5.6348 + 2.3509x_{1k} +  1.7514x_{2k})}}$$\n\nfor $(k = 1, \\ldots, K)$.\n\n__Example:__ for the $4th$ person in our dataset, i.e. a woman with income of 2.54 (thousands) the following can be derived:\n\n\\begin{equation}\n\\begin{split}\nz &= -5.6348 + 2.3509 \\cdot 2.54 +  1.7514 \\cdot 0 = 0.3365 \\\\\n\\Rightarrow p &= \\frac{1}{1+e^{-0.3365}} = 0.5833\n\\end{split}\n\\end{equation}\n\nAlso, the positive coefficient for gender indicates that male ($=1$) tend to have a higher probability of buying the product. This can be confirmed using our model and assuming a male person that has equal income than our female example (i.e. $2.54$):\n\n\\begin{equation}\n\\begin{split}\nz &= -5.6348 + 2.3509 \\cdot 2.54 +  1.7514 \\cdot 1 = 2.0879 \\\\\n\\Rightarrow p &= \\frac{1}{1+e^{-2.0879}} = 0.8897\n\\end{split}\n\\end{equation}\n\nLooking at the ROC curve we can see that the model looks better. \n\n## Estimation of logistic regression function\n\nUnlike for the linear regression model there is no analytical solution to estimating the correct coefficients. This is due to the non-linearity of the function. When calculating the optimal logistic regression function we apply maximum likelihood method (MLE). When applying maximum likelihood we are estimating the coefficients such as the given data is most likely (i.e. plausible). \n\nThis means that for a person k in our model the probability $p(x_k)$ should be high if $y_k = 1$ (low for $y_k = 0$). Mathematically this can be expressed as:\n\n$$p(x_k)^{y_k}\\cdot (1-p(x_k))^{1-y_k}$$\n\nOne assumption of the model is that $Y_k$ is independently distributed for all observations. This means that we can express the total probability over all observations as the product of individual probabilities. This yields the following __likelihood function__ that needs to be maximized:\n\n$$L(\\beta) = \\prod_{k=1}^K p(x_k)^{y_k}\\cdot (1-p(x_k))^{1-y_k} \\rightarrow Max!$$\n\nThe $\\beta$ parameter need to be estimated such that the likelihood is maximized. In practice it is more feasible to use the logarithm of the above function given we can replace the product with a sum which is simpler:\n\n$$LL(\\beta) = \\sum_{k=1}^K ln[p(x_k)]\\cdot y_k + ln[1-p(x_k)]\\cdot(1-y_k) \\rightarrow Max!$$\n\nThe solution to this maximization effort is found using _iterative approches_ such as gradient descent. \n\n## Interpretation of regression coefficiens\n\n### Caution when interpreting coefficients\n\nGiven the nonlinearity of the logistic regression model it is somewhat more difficult to interpret the individual coefficients from our model optimization. One of the main challenges is that the impact of a coefficient is not constant but varies with the dependent variable. \n\nTo this end, it is only possible to state: _how much the change of one parameter, changes the dependent variable_. However, it is not possible to state by how much the dependent variable changes as the slope of changes not constant.\n\nChange of __intercept__ (i.e. $\\beta_0$):  \n- changes curve horizontally\n- increase of $\\beta_0$ increases probability for a given $x$ (and vice versa)\n\nChange of __slope__ (i.e. $\\beta_1$):\n- increase of coefficient yields steeper curve in the mid range\n- $\\beta_1 = 0 $ yields a flat curve\n- change in sign changes slope of curve (if negative slope is downward)\n\n<br>\n\n__Difficulty of interpretation:__ the impact of a coefficient is not constant on $Y$\n\n- for linear regression we could say: a change in $x$ changes $y$ by $\\beta$\n\n- for logistic regression this change also is dependent on $y$ (i.e. the probability p). The biggest impact is when $p=0.5$\n\n\n(Note: show interactive example; examples can be seen in [*Backhaus et al (2017)*](https://www.springer.com/de/book/9783662460764), p. 291ff\n\nWe can illustrate the fact that the impact of a coefficient is dependent on p (and is highest for $p=0.5$) by looking at our previous simple logistic regression model:\n\nHere we calculated a probability of $\\approx 50$% for an income of $2$ (i.e. 2.000 EUR).\n\nWe can now calculate the values for different incomes and see that the change in probability will decrease, i.e. the impact of the coefficient becomes less the higher the probality gets.\n\n### Odds-Ratio and Logit as help for interpretation\n\nThe logistic model can alternatively be expressed in _odds_ and _logits_ both of which simplifies interpretation. \n\nOdds and logit are defined as follows:\n\n\\begin{equation}\n\\begin{split}\n\\\\[1.5ex]\n\\text{Odds} &= \\frac{p}{1-p} = e^{\\beta_0 + \\beta_1x} =  e^{-3.6707 + 1.8272x} \n\\\\[3ex]\n\\text{Logit} &= ln\\big(\\frac{p}{1-p}\\big) = \\beta_0 + \\beta_1x =  -3.6707 + 1.8272x\n\\end{split}\n\\end{equation}\n\n#### Odds\n\n__Odds__ can be interpreted as chance of winning and are calculated as a relation of probability $p$ and its counter probability $(1-p)$. Let's say the probability of winning is $75$% the odds are:\n\n$$odds = \\frac{p}{1-p}=\\frac{0.75}{1-0.75}=4$$\n\nmeaning that there is a $4$ in $1$ chance of winning. Odds cannot become negative (afterall the lowest chance of winning is zero) but are not constraint to $1$. We can also derive the probability $p$ from the odds by\n\n$$p = \\frac{odds}{odds + 1} = \\frac{4}{4+1} = 0.8$$\n\n\n__Why is odds helpful for interpreting the coefficients?__\n\nWe want to understand how $y$ changes with a change in $x$. In order to understand why _odds_ may help let's calculate the odds for $x+1$ (i.e. $x$ changes by 1). \n\n\\begin{equation}\n    \\text{odds}(x+1) = e^{\\beta_0 + \\beta_1(x+1)} = e^{\\beta_0 + \\beta_1x + \\beta_1} =  e^{\\beta_0 + \\beta_1x} \\cdot e^{\\beta_1} \n    = \\text{odds}(x) \\cdot e^{\\beta_1}\n\\end{equation}\n\nWe can now caluclate the ratio of odds from odds(x+1) and odds(x) and get:\n\n$$OR = \\frac{\\text{odds}(x+1)}{\\text{odds}(x)} = e^{\\beta_1}$$\n\n> __Conclusion__  \n> In other words, __odds increase by factor__  $e^{\\beta_1}$ when increasing $x$ by one. \n\nIn our example this means that odds increase by $e^{1.8272}=6.216$ when increasing income by $1$ (i.e. 1.000). This means that the odds increase by a constant factor (not a constant value as with linear regressio). In our case the factor is 6.216 which translates into a probability of $\\frac{6.216}{1+6.216} \\approx 86$%. \n\n> **Interpretation**  \n> To this end, when income is increased by $1$ unit the probability of a purchase is increases by $86$%.\n\nNumerical example to show that odds stay constant:\n\nNote that for $\\beta_1 = 0$ the __odds ratio (OR)__ becomes 1.\n\n#### Logit\n\nLogit is short for _logarithm of odds_ (i.e. logarithmic odds or log-odds) given it is true that:\n\n$$logit(p) = ln(odds(p)) = ln\\bigg(\\frac{p}{1-p}\\bigg) = ln(e^{z}) = z \\Rightarrow \\beta_0 + \\beta*x$$\n\n\nIn doing so we transform the range of possible values from $[0,1]$ to $[-\\infty, \\infty]$. In line with want we have done for _odds_ we will try to understand what happens to $y$ if we increase $x$ by $1$, i.e. increase to $x+1$:\n\n\\begin{equation}\nlogit(p(x)) = \\beta_0 + \\beta_1*(x+1) = \\beta_0 + \\beta_1x + \\beta_1\n\\end{equation}\n\nGiven $\\beta_1$ is $1.8272$, $logit(p(x))$ increases by $1.8272$. The beta coefficient from a logistic regression model can therefore be interpreted as __the marginal effect on the logit__. Logit increases by $\\beta_1$ if income is increase by $1$ unit. \n\nGiven we don't think in logits it is somewhat harder to interpret the logits.\n\n## Assessment of overall model\n\nWe already know from our linear regression chapter that it is necessary to understand the goodness of fit for the overall model.\n\nIn Section 3 we have learned that - when estimating the model - we are maximizing the log likelihood (LL). One approach could therefore be to take the value of the LL as a indicator for the goodness of fit. \n\nIn our multiple logistic model the log-likelihood is:\n\nIt is common to use not the LL but use $-2\\cdot LL$ instead. The smaller the value the better the model is (note that -2 stems from the fact that we are assuming a chi-squard distributed test statistic).\n\nFor our model the $-2LL$ is \n\nProblematic about this value is that it is dependent on the number of observations (remember a similar issue with RSS in linear regression). This means the higher the number of observations, the higher the $-2LL$ (ceteris paribus). \n\nHowever, the $-2LL$ can be helpful when comparing different models with each other. \n\nFor example, we can compare the LL between our two models:\n\n\\begin{equation}\n\\begin{split} \nz_1 &= \\beta_0 + \\beta_1 \\cdot income \\\\\nz_2 &= \\beta_0 + \\beta_1 \\cdot income + \\beta_2 \\cdot gender\\\\\n\\end{split} \n\\end{equation}\n\nWe see that the second model is better as its $-2LL$ is actually lower. \n\nAs an extreme case we could also create a model where we only include a constant, i.e. \n\n\\begin{equation}\n\\begin{split} \nz_3 &= \\beta_0 \n\\end{split} \n\\end{equation}\n\nIf we run the model we see that the $-2LL$ is even higher. \n\nThe constant or 0-Modell can be used to construct a test statistic called _likelihood-ratio-test_. \n\n### Likelihood-Ratio-Test (LR test)\n\nThe LR test can be calculated as follows:\n\n\\begin{equation}\n\\begin{split}\nLLR &= -2 \\cdot ln\\bigg(\\frac{LL_0}{LL_{complete}}\\bigg) \\\\[3ex]\n&= -2 \\cdot (LL_0 - LL_{complete})\n\\end{split}\n\\end{equation}\nwhere:\n\n$LL_0 : $ maximum log likelihood for model only including a constant and  \n$LL_{complete} : $ maximum log likelihood for model including all required variables (in our case income and gender)\n\nFor our example the $LLR$ is therefore calculated as:\n\n$$LLR = -2*(-20.728 + 16.053) = 9.350$$\n\nUnder the null hypothesis $H_0 : \\beta_1 = \\beta_2 = \\ldots = \\beta_J = 0$ the LLR is approximately $\\chi^2$ distributed (with $J$ degrees of freedom). \n\nWe can therefore say that $\\chi_{emp}^2$ = 9.350. In line with previous significance tests we can now calculate the theoretical $\\chi^2$ given (for example) an $\\alpha$ of 0.05 and 2 degrees of freedom (we have 2 explanatory variables). Doing so yields a $\\chi^2$ of $5.99$. \n\nGiven $9.350 > 5.99$ we can conclude that our model is statistically significant. \n\nAternatively, we can directly calculate the p-value by calculating:\n\nThe model is statistically highly significant since the p-value is well below $1$%\n\n### Pseudo $R^2$\n\nWe have learned about $R^2$ score for linear regresssion which helps to understand which part of the deviation in $y$ is explained by our model. \n\nUnfortunately such a measure does not exist in the case for logistic regression. There is no measure that separates the expected from the unexpected deviation in the case of logistic regression. \n\nThere are, however, alternative measures which attempt to provide a similar meaning: the pseudo $R^2$ measures. They attempt to create a similar interpretation as the $R^2$ measure, i.e. \n\n- can take a range between 0 and 1\n- the higher the measure the better the model\n\nThis is achieved by calculating probability ratios instead of deviation ratios (as with $R^2$). \n\n__McFadden's $R^2$__: is defined as\n\n$$\\text{McF-}R^2 = 1-\\bigg(\\frac{LL_{complete}}{L_0}\\bigg) = 1 - \\frac{-16.053}{-20.728} = 0.226$$\n\nWe can easly confirm this using our previously calculated log-likelihood values:\n\nGiven in practice it is very unlikely that the log-likelihood ratios become 0, as a rule of thumb values between $0.2$ and $0.4$ already indicate a good model fit. \n\n## Assessment of coefficients\n\nIn line with regression analysis we have to test if individual coefficients are statistically significant. We do so by testing the null hypothesis $H_0 : \\beta_j = 0 $.\n\nIn linear regression we have done so by using a _t-test_.\n\nIn the case of logistic regression we use \n\n- Wald test and/or\n\n- likelihood ratio test\n\nGiven the fact that most statistical packages readily provide calculations of significance levels of individual coefficients, we refrain from looking into both tests in detail. \n\nInstead let's look at the summary output for our final model and start to interpret the output.\n\nWe can see that values for the overall model are confirmed by the summary output. In addition we see that the coefficient _income_ is statistical significant at the 95% conficence interval (p value of 2.4%), whereas the coefficient _gender_ is not statistically significant (p value of 6.6%). \n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"S07a_Classification.html"},"language":{},"metadata":{"lang":"de","fig-responsive":true,"quarto-version":"1.2.269","theme":"cosmo","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}}}