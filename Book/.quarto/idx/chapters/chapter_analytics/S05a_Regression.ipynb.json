{"title":"Lineare Regression","markdown":{"headingText":"Lineare Regression","containsRefs":false,"markdown":"\n\nThe following chapter is based on [*Backhaus et al (2017)*](https://www.springer.com/de/book/9783662460764).\n\n## Introduction\n\nLinear regresssion is one of the most flexibel and widely used statistical methods used in research. It is used for analysing the relationship between a **dependent** and one or more **independent** variables. Linear regression is used for \n\n1. **inference**, i.e. to test a prior developed hypothesis about the relationship between variables of interest\n\n2. **prediction**, i.e. to estimate the value of a dependent variable given values of independent variables\n\nPrimary use case for linear regression analysis is the analysis of **causal relations**. This relation can be expressed as \n\n$$Y = f(X)$$\n\n**Simple (linear) regresssion:**  \n\n\nIf we want to express that there we believe in a relation between *revenues* and *price* we may state this as\n\n$$\\text{revenues} = f(\\text{price})$$\n\nUsing linear regression this **relation can be quantified**, i.e. we can determine how much *revenue* will change if we change *price*. \n\n\n**Stochastic model:**    \nIt is very unlikely that the the relation between the above variables is entirely deterministic (as assumed in the above formula). We, therefore, need to introduce uncertainty to the model. The resulting **stochastic model** is commonly used in regression analysis and is described as:\n\n$$Y = f(X) + \\epsilon$$\n\nHere $\\epsilon$ is a **random variable** (called error term / residual) which cannot be observed and is assumed to follow a standard normal distribution (i.e. $\\epsilon \\sim N(0,1)$). The stochastic model will be required to **evaluate the regression models using statistical tests**.\n\n\n**Multiple (linear) regression:**  \nIn many (if not most) research questions we may not assume a monocausal relationship. Instead $Y$, our variable we want to analyse, is influenced by numerous factors. In our above example *revenues* may also depend on advertising spent but also from other factor such as the state of the economy, the price, behaviour of competitors etc. \n\nFor such a relationship we use a **multiple regression analysis** which may be expressed as:\n\n$$Y = f(X_1, X_2, \\ldots, X_J)$$\n\n\n**Causality vs. correlation:**  \nIt is important to state that while we attempt to model a causal relationship between $Y$ and $X$, in practice we cannot determine whether or not the relationship is actually causal. Instead all we do is we approximate causality be assessing correlation. \n\n**Typical hypotheses modelled with linear regression**\n\n | # | Hypothesis                                                                                                | Dependent variable                    | Independent variable                                    |\n|---|-----------------------------------------------------------------------------------------------------------|---------------------------------------|---------------------------------------------------------|\n| 1 | Is revenue by salesperson dependent on number of customer visits?                                         | Revenue per sales person (per period) | Number of customer visits per sales person (per period) |\n| 2 | Will sales change if advertising spent is doubled                                                         | Sales per period                      | Spent on advertisment per period                        |\n| 3 | How does a price increase of x% impact sales if advertisment spent is   increased by 10% at the same time | Sales per period                      | Advertisment spent, price, â€¦                            |\n|4|...|...|...|\n\n\n\n## Model formulation\n\nA model is a simplified representation of reality. Models are very useful but it is always a fine line between simplification and complexity. If we want to model reality as close as possible our model may become to complex. If our model is to simple it may not describe reality good enough for our purposes. There is no such thing as a good or a bad model. It is more helpful to think of a model as suitable or not suitable for our problem at hand. It turns out the linear regression models are quite simple yet very suitable for many research and practical problems which is why they are heavily relied on in all social sciences. \n\n> **Dataset**: In the following we will be using the advertising dataset from [Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/index.html) for the following examples. The dataset contains   \n> - sales in thousands units\n> - advertising budgets in thousand of dollars for TV, radio and newspaper\n\n**Sales vs. TV advertisment spent:**  \nLet us assume that we believe in a simple linear relation between **sales** and **TV** advertisment spent. We can describe this as  \n\n$$\\text{sales} = f(\\text{TV})$$\n\nThis implies that we believe in a causal relationship between both variables. Specifically we believe that *TV* advertisment spent drives or influences *sales*. \n\n**Regression function:** \n\nA simple linear regression model of the above formulations could be:\n\n$$\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X $$\n\nwhere $\\hat{Y}$ denotes the prediction of the dependent variable $Y$ given $X$. $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are the coefficient estimates. \n\nFor our example the regression function looks as follows:\n\n$$\\text{sales} = \\hat{\\beta}_0 + \\hat{\\beta}_1\\text{TV}$$\n\nGiven the mathematical formulation implies a line in a 2D field, $\\beta_0$ represents the intercept of the line with the y-axis and $\\beta_1$ represents the slope of the line. \n\n\n\n\n<img src=\"https://www.dropbox.com/s/g8smbktpeqgl90u/regressionmodel.png?dl=1\" alt=\"regression_model\" width=\"50%\" align=\"center\"/>\n\nLet's assume that $\\beta_0$ is $20$ and $\\beta_1$ is $0.5$. This would mean that our regression model becomes:\n\n$$\\text{sales} = 20+ 0.5\\text{TV}$$\n\nThis would mean we could insert X and calculate $\\hat{Y}$. \n\nFor example, if we assume we spent 100 in TV advertisment we would expect sales of \n\n$$\\text{sales} = 20 + 0.5\\times100 = 70$$\n\nSpending 100.000 USD in TV advertisment would yield in 70.000 Units of sales according to our model. \n\n(Note: sales and TV spent both in in units of thousands)\n\n**How do you determine coefficients?**  \nIn our example we have just assumed values for $\\beta_0$ and $\\beta_1$. In practice we need to determine both coefficients. In order to derive how we do this it is helpful to look at the following interactive example:\n\n[Interactive example](https://share.streamlit.io/fredzett/rmqa/regression.py)\n\n\n\n\n## Estimation of regression function\n\n### Simple linear regression\n\nWe need to find coefficients that specify a line which describes the true relationship as best as possible. \n\nThis is achieved by minimizing the *least squares* criterion, i.e. we want to minimize the resdiual sum of squares (RSS; sometimes SSR)\n\n$$\n\\text{RSS} = \\sum_{i=1}^n e_i^2\n$$\n\nwhere $e_i$ is defined as \n\n$$\ne_i = y_i - \\hat{y}_i\n$$\n\nThe **least squares criterion** avoids that positive and negative deviations cancel each other out and weighs more heavily on large deviations (which may also be a disadvantage if we find to have outlier in our data). \n\n\nAnalytically we can then derive $\\beta_0$ and $\\beta_1$ that minimizes the RSS:\n\n\\begin{equation}\n\\begin{split}\n\\hat{\\beta}_1 & = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\\\[10pt]\n\\hat{\\beta}_0 & = \\bar{y} - \\hat{\\beta}_1\\bar{x}\n\\end{split}\n\\end{equation}\n\nWe write $\\hat{\\beta}_i$ to indicate that it is an estimator. We omit if this is clear from the context. \n\nWe can easily implement the above formulas in python and calculate the correct values for both coefficients\n\nThe optimal regression line can therefore be described as:\n\n$$\\text{sales} = 7.03 + 0.048\\times\\text{TV}$$\n\nmeaning that increasing TV spent by 1.000 USD will increase sales units by 48 (remember that both are in units of thoushands)\n\nLooking at our [interactive chart](https://share.streamlit.io/fredzett/rmqa/regression.py) we see that the line does not perfectly describe the relationship given most of the data points are not on the line described by the regression model. This is due to the fact that there are other factors influencing *sales*. We distinguish between two types of factors:\n\n1. **systematic factors**, i.e. other explanatory variables (e.g. price, newspaper ad spent, ...) which may drive sales but have not been incorporated into the model\n\n2. **unsystematic factors**, i.e. other factors which cannot be included because they are unknown\n\n### Multiple regression\n\nFor most research question it is required that we have more than one indepenent variable. If this is the case the regression model has the following form:\n\n$$\\hat{Y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_Jx_J$$\n\n**Example:**  \nWe could describe the relationship between *sales* and advertising spent using all three media types, *TV*, *Newspaper* and *radio*. This would then be described as:\n\n$$\\text{sales} = \\beta_0 + \\beta_1\\text{TV} + \\beta_2\\text{newspaper} + \\beta_3\\text{radio}$$\n\n\nIn order to solve for $\\beta_j$ we, in principal, follow the same route than in the simple regression case. However, solving the system of equations is somewhat more complex. It involves solving a system of linear equations of the following form:\n\n$$Y = X\\beta + \\epsilon$$\n\nwhere:\n\n\\begin{equation}\nY = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}, \nX = \\begin{pmatrix} 1 & x_{11} & x_{12} & \\ldots & x_{1p} \\\\ 1 & x_{21} & x_{22} & \\ldots & x_{2p} \\\\ \\vdots & \\vdots &  \\vdots  & \\vdots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\ldots & x_{np} \\end{pmatrix},\n\\beta = \\begin{pmatrix} \\beta_0 \\\\ \\vdots \\\\ \\beta_p  \\end{pmatrix}, \n\\epsilon = \\begin{pmatrix} \\epsilon_0 \\\\ \\epsilon_1  \\\\ \\vdots \\\\ \\epsilon_n  \\end{pmatrix}, \n\\end{equation}\n\nWithout covering the details of the math this equation can be solved as follows:\n\n$$\n\\hat{\\beta} = (X^TX)^{-1}X^Ty\n$$\n\nWhat is important here is that there exists an analytical solution for the linear regression problem. \n\nThe implementation in python is easy yet not intuitive. \n\nAlso note that in the multiple regression part is it necessary to assume one factor to be consisting of $1$s in order to calculate $\\beta_0$ (i.e. the intercept)\n\n**Implementation in python**:  \nIt is tedious and verbose to implement regression from scratch. We will therefore use existing modules to achieve this. In doing so we can also make use of many statistical tests that are calculated on the fly. \n\nWe will use:\n\n- `patsy.dmatrices`: this produces X and y based on a model specification (use model `patsy`)\n- `statsmodels.OLS`: calculates an ordinary least squares regression (use model `statsmodels`)\n\nThis is very helpful as we can also use the optimized model (i.e. variable `ols`) to print summary statistics for the entire  regression model. \n\nWe will use the combination of both modules / functions to make our lives much easier and be able to explore models.\n\n**Meaning of regression coefficients:**  \nWe have now seen how to calculate the regression coefficients that minimize the residual sum of squares (RSS). Let's now focus on how to interpret these coefficients:\n\nThe coeffiecients give the **marginal effect of change** of an independent variable on the dependent variable. For example, a coefficient of `0.18` for `radio` means that increasing radio spent by a $1.000$ USD - and keeping all other variables constant - the sales increase by $180$.\n\nIt is, however, important to understand that the **size of the coefficient does not indicate importance** and should not be interpreted directly. For example, it cannot be concluded from the above estimation that the effect of *TV* is less than the effect of *radio* if we do not know the scale of the variables. More specifically, the scales of the variables need to be of the same size to make such a statement valid. In the above case it is actually valid to make such a statement because all independent (i.e. explanatory) variables are measures in '000 USD. \n\nFor most models, the scale between different independent variables differs, however. \n\nLet's look at a different dataset containing information about different car models. The dataset has the following variables:\n\n- `mpg` = miles per gallon\n- `cylinders` = number of cylinders \n- `displacement` = overall volume in the engine as a factor of cylinder circumfrance, depth and total number of cylinders. This metric gives a good proxy for the total amount of power the engine can generate.\n- `horsepower` = gross horsepower\n- `weight` = weight of the car in lbs\n- `acceleration`= time in seconds to 100mph\n- `year` = year model was produced\n- `origin` = region (1=US, 2=Europe, 3=Asia)\n- `name` = name of model\n\nJust be looking at the descriptions it becomes apparent that the units of each variable are not comparable. \n\nNow let's assume we want to understand the drivers for `mpg`, i.e. what are factors influencing mileage per gallon. We hypothesize about a model that looks as follows:\n\n$$\\text{mpg} = \\beta_0 + \\beta_1\\text{horsepower} + \\beta_2\\text{weight} + \\beta_3\\text{year}$$\n\nIntuitively we would assume that `mpg`:\n- decreases with more `horsepower` and `weight` (as big and/or heavy cars usually are less efficient)\n- increases with `year` (as newer cars are more efficient)\n\nOur initial intuition was confirmed given the negative signs of coefficients for `horsepower` and `weight` and the positive coefficient for `year`.\n\nWe can now make a statement that $100$ horsepower extra decreases mileage per gallon by $5$. \n\nWe may, however, be interested in the **most influential factor**. What is driving `mpg` the most? In this example we cannot state that `year` is driving horsepower the most given the units and scales of all three variables are not comparable.\n\n**Standardized coefficients**  \nIn order to answer the question with of the $X$s is most important for determining $Y$ we need to make our $\\beta$s comparable. We can do this following two ways:\n\n1. Standardize variables using *z score*\n\n2. Standardize coefficients \n\n**Z-Score approach**\n\n- standardize the variables using a z-score (i.e. mean of 0 and standard deviation of 1 for each variable)\n- run ols regression and receive standardized coefficients\n\nNotice that the optimal parameters have changed. We can now compare the coefficients and say that \n\n- horsepower has the least impact\n- year has the second least impact\n- weight has the most impact and it's impact is 2x as high as the impact for year\n\nIt is recommended to use the below approach as it avoids\n\n- rerunning the entire model with new variables\n- leaves $X$s unchanged\n\n**Standardized coefficients approach**\n\n- run ols regression using nominal (i.e. original) values for variables\n- convert nominal coefficients using $$\\beta_{j,std} = \\beta_j \\cdot \\frac{s_{X_j}}{s_Y}$$\n\nwhere\n\n$\\beta_{j,std}$ = standardized coefficient for variable $X_j$  \n$\\beta_j$ = unstandardized / nominal coefficient for variable $X_j$  \n$s_{X_j}$ = standard deviation of $X_j$  \n$s_Y$ = standard deviation of $Y$\n\nLet's do this in python\n\nWe can see that the resulting coefficients are different from the z-score approach. However, the interpretation yields exactly the same conclusion:\n\n- horsepower has the least impact\n- year has the second least impact\n- weight has the most impact and it's impact is 2x as high as the impact for year\n\n**Interpretation of standardized coefficients**  \nWe can say that changing $X_j$ bei $1$ standard deviation changes $Y$ bei $\\beta_{j, std}$ standard deviations.\n\nExample: changing `year` by $1$ standard deviation changes `mpg` by $0.35\\ldots$ standard deviations. This means \n\nWe can confirm this is true by comparing to the original parameter\n\nSummarizing:\n\n- nominal coefficients: use to show \"real unit\" effect of $X$ on $y$\n- standardized coefficients: use to compare effect size of $X$s on $y$\n\n## Evaluation of regression function\n\nWe have esimtated our regression function. However, we don't know how good the model. We have used a model to approximate reality and we need to understand how good our model is in doing so. We therefore need to evaluate our model. This consists of two types of evaluation:\n\n1. **Global assessment**, i.e. how good can $Y$ be explained by $X$ \n\n2. **Coefficient assemssment**, i.e. if and how individual $X$s contribute to explaining $Y$\n\n### Global assessment of regression model\n\nThe global assessment, i.e. the goodness of fit, can be done using two different metrics:\n\n1. $R^2$\n\n2. $F$-Statistic\n\nWe have seen that we optimize our regression model by minimizing\n\n$$\n\\text{RSS} = \\sum_{i=1}^n e_i^2\n$$\n\n\nUnfortunately, we cannot use RSS as an indicator of how good our model is as it is dependents on the size of the dataset. The larger the dataset the higher the RSS. \n\nHowever, we can disassemble the RSS in \n\n- explained deviation and\n- unexplained deviation\n\nLet's look at the deviations in the simple linear regression case with one explanatory variable $X$ and the dependent variable $Y$.\n\n<img src=\"https://www.dropbox.com/s/rz386z8fvl43o6o/regression_deviation.png?dl=1\" alt=\"regression_model\" width=\"50%\" align=\"center\"/>\n\n\n\nLet's analyse one single observation $x_i$ with its corresponding observation $y_i$. The point $(x_i, y_i)$ is above the mean $\\bar{y}$. We treat $y_i - \\bar{y}$ as the total deviation. This deviation can be separated into:\n\n- explained deviation: $\\hat{y}_i - \\bar{y}$; given $x_i$ is higher than $\\bar{x}$ we expect $\\hat{y}_i$ to be higher than $\\bar{y}$\n\n- unexplained deviation: $\\epsilon_i$; cannot be exlained by the above argument\n\nWe can therefore say that: *total deviation = explained deviation + residuum*\n\nWhich can trivially be written as:\n\n$$y_i - \\bar{y} = (\\hat{y}_i - \\bar{y}) + (y_i - \\hat{y}_i)$$\n\nThis equation also holds for multiple regressions.\n\n**Definition of $R^2$**  \nIs derived from this relationship and is defined as \n\n$$R^2 = \\frac{\\text{explained deviation}}{\\text{total deviation}} = \\frac{\\sum_{i=1}^I (\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^I (y_i - \\bar{y})^2}$$\n\nThe $R^2$ score can be interpreted as how much % can be explained by the specified model. (Note: altenatively, the $R^2$ can also be calculated as the square of the correlation between $Y$ and $\\hat{Y}$).\n\nLet's look at our original advertisment model and see how good the model actually is:\n\nAlternatively we can calcualte it as the square of correlation between $Y$ and $\\hat{Y}$,\n\nEither way, this means that $\\approx 60\\%$ of sales deviation can be explained by spent in TV advertisment.\n\nWhile the $R^2$ is a good metric it has some drawbacks, mostly:\n\n- does not account for size of dataset: for example the $R^2$ of two data points is always 1 given we can always connect them using a line\n- does not account for number of $X$s: complex models with many variables always have a higher $R^2$ (all other things being equal)\n\nTo compensate for these drawbacks we can use \n\n- adjusted $R^2$\n- $F$-statistic\n\n**Adjusted $R^2$**  \nTakes the number of variables into account and adjusts for it by \n\n$$R^2_{adj} = R^2 - \\frac{J \\cdot (1 - R^2}{N- J - 1}$$\n\nwhere:\n\n$N$ = number of observations (i.e. elements in dataset)  \n$J$ = number of coefficients (i.e. variables in datatset)  \n$N - J - 1$ = number of degrees of freedom\n\nComparison of $R^2$ and $R^2_{adj}$ using our advertisment dataset\n\n**F-Statistic**\n\nThe $R^2$ measures how much of $Y$ can be explained by the model. Given in most research (and practical) context our data is based on a sample and not a population we have to understand to what extend the model - based on the sample data - is valid for the population. Therefore, we need to **understand the statistical significance of the model**. This is what the $F$-statistic is used for. \n\nThe $F$-statistic includes\n\n- the deviations explained above\n- the number of observations\n- the number of coefficients \n\nGiven the estimated regression function (of a sample)\n\n$$\\hat{Y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_Jx_J + \\epsilon$$\n\ncan be interpreted as \"true\" function with unknown parameters $\\beta_0, \\beta_1, \\beta_2, \\ldots,\\beta_J$ which describes the causal realtionship of the population. This function includes an error term ($\\epsilon$). Therefore, the regression function can be regarded as a **stochastic model**.\n\nAs discussed $\\epsilon$ includes all random factors which drive the dependent variable $Y$. This variable cannot be observed directly (given we don't know which unsystematic factors influence $Y$ or else we would have included them) but is measured by the **residuals**. \n\nIt follows from the above statement that $Y$ is also a random variable and the the coefficients (i.e. $\\beta$s) are also realisation of random variables. Therefore with different samples these coefficients will vary around the true value of $\\beta$.\n\nIf a causal relationship exists betwenn $Y$ and $X$ then the true regression coefficients must be different from zero. \n\nTo this end, for testing the significance of the regression model the following **null hypotheses must be tested**:\n\n$$H_0 : \\beta_0 = \\beta_1 = \\ldots = \\beta_J = 0$$\n\nFor testing this hypothesis we conduct the $F$-Test using the $F$-distribution doing the following:\n\n- calculate the empirical $F$-value from the model \n- provide significance level\n- compare to the theoretical $F$-value given the $F$-distribution / calculate p-values\n\n**Calculation of empirical $F$-value**:\n\n$$F_emp = \\frac{\\sum_{i=1}^I(\\hat{y}_i - \\bar{y})^2/J}{\\sum_{i=1}^I(y_i - \\hat{y}_i)^2/(I - J - 1)} = \\frac{\\text{explained deviation}/J}{\\text{unexplained deviation}/(I - J - 1)} = \\frac{R^2/J}{(1 - R^2)/(I - J - 1)}$$\n\nwhere:\n\n$I$ = number of observations  \n$J$ = number of parameters (coefficients)\n\n\nExample using the `car dataset`\n\n**1. calcualte empirical f-value**\n\nWe can use the function from our model. However, let's check if the value can actually be recalculated using the above formula:\n\nWe will use the function from statsmodels from now on.\n\n**2. determine significance level**\n\n**3. compare to theoretical f-value**\n\nInterpretation: our model yields an empirical $F$-value that is very unlikely under the theoretical $F$-distribuation. Therefore, we can conclude that at least one of the parameters ($\\beta$s) is not equal to zero. This means that we actually can be quite certain that we have specified a model that found a (statistical) realtion between $Y$ and $X$.\n\n### Assessment of regression coefficients\n\nWe have so far \"only\" tested the significance of the entire model. However, we have not tested the individual coefficients. \n\nThe common hypothesis we need to evaluate is:\n\n$$H_0 : \\beta_j = 0$$\n\n(note that we could in principal also conduct other hypotheses)\n\n**$t$-statistic**\n\nWe have seen in previous chapter how to conduct the $t$-test. The steps are the same as with the $F$-test.\n\n1. we calculate the emprical $t$-value\n2. we provide significance level\n3. we compare to the theoretical $t$-value given the $t$-distribution / calculate p-values\n\nIf we test the null hypothesis as described above the emprical $t$-value can be determined using:\n\n$$t_\\text{emp} = \\frac{\\beta_j}{s_{\\beta_j}}$$\n\nwhere:\n\n$s_{\\beta_j}$ = the standard error of $\\beta_j$.\n\nThe calculation of the standard error in the multiple regression case is somewhat more complex and will not be covered here. This is due to the fact that\n\n1. most statistical packages yield the standard errors\n2. we could calculate it using a bootstrap approach\n\nFor further information on the calculation of standard errors see, e.g., [*Backhaus et al (2017), p. 122ff*](https://www.springer.com/de/book/9783662460764).\n\nExample: using the above model\n\n**Confidence intervals using bootstrap approach**\n\nUsing the `summary` function we can, however, make this analysis less verbose. We also get other statistics such as \n\n## Validating model assumptions\n\nWe have so far not discussed the assumptions that we depend on when calculating test statistics etc. For calculating these statistics we mainly rely on the residual which is stochastic. We introduced this term to incorporate uncertainty into our model which must be included given that we are estimating a model (and its parameters) based on a sample (see last chapter). When conducting the regression analysis we make multiple assumptions regarding the stochastic component. \n\n**Assumptions**:\n\n1. the model is correctly specified\n    - it is linear for the parameters ($\\beta$s)\n    - it includes the relevant explanatory variables\n    - the number of estimated parameters (J + 1) is smaller than the number of observations (I)\n\n2. the expected value of $\\epsilon$ is zero\n3. the correlation between the explanatory variables and $\\epsilon$ is zero\n4. the variance of $\\epsilon$ is constant (called *homoscedasticity*)\n5. there is no autocorrelation in $\\epsilon$\n6. there is no linear dependency between the explanatory variables $X$\n7. the residuals are normally distributed\n\nIf assumptions 1 to 6 hold the linear regression model yields unbiased estimates (best linear unbiased estimator, BLUE).\n\nFor conducting the significance tests ($F$- and $t$-test) assumption 7 needs to hold. \n\nPlease refer to [*Backhaus et al (2017), p. 89ff*] for furter details on testing model assumptions.\n\n## Non-linearity\n\nA common misconception with linear regression is that the relation between $Y$ and $X$ needs to be linear. This is actually not the case. Assumption A1 only stipulates for linearity in the parameters. It is possible in many cases to transform non-linear relationships into linear relationships. \n\nGiven the model \n\n$$Y = \\beta_0 + \\beta_1X + \\epsilon$$\n\nwe can replace $X$ with $X' = f(X)$, where $f$ is a nonlinear function. The resulting model:\n\n$$Y = \\beta_0 + \\beta_1X' + \\epsilon$$\n\nis still linear in its parameters.\n\nThe most common nonlinear transformations in using linear regression is the log-transformation. \n\nRemember that $$log(a) = b \\leftrightarrow a = e^b$$ \n\n\nWe can distinguish between four model types:\n\n1. level-level model: $Y = \\beta_0 + \\beta_1X + \\epsilon$\n\n2. log-level model: $log_e(y) = \\beta_0 + \\beta_1X + \\epsilon$\n\n3. level-log model: $Y = \\beta_0 + \\beta_1log_e(X) + \\epsilon$\n\n4. log-log model: $log_e(y) = \\beta_0 + \\beta_1log_e(X) + \\epsilon$\n\nWhen using models 2 to 4 it is important that the interpretation of the coefficients differs from the *level-level* model. Interpretations are as follows:\n\n- log-level: increasing $X$ by one unit changes $Y$(on average and approximately) by $100 \\cdot \\beta_1\\%$\n\n- level-log: increasing $X$ by one percent, changes $Y$ (on average) by $\\beta_1/100$ units\n\n- log-log: increasing $X$ by one percent, changes $Y$ (on average) by $\\beta_1\\%$\n\nIn order to determine if it is helpful to transform data we should **inspect it visually**. \n\nWe can see that `mpg` and `horsepower` seems to be non-linear. \n\nWe could therefore decide (and test) to transform one or both of the variables. \n\nLet's see how that works.\n\nLet's run a regression analysis on both models\n\nJust looking at the two graphs we can see that the second model (log-log) seems to be better and capturing / explaining the deviation. We can also confirm this when looking at the $R^2$ value which is $72$% vs. $61$%.\n\n## Interaction effects\n\nA special case of non-linearity occurs when we combine two or more variables multiplicatively. This may be necessary or feasible if one effect depends (or is conditional) on another effect. \n\nLet's return to our `advertisment` data set where we analyzed the causal relationship between `sales` and advertisment spent on `TV`,  `newspaper` and `radio`.\n\nThe model assumes that the effect of increasing advertisment spent in one medium (e.g. TV) is independent of the amount spent on the other media. The effect on `sales` of spending one additional unit on `TV` is always $\\beta_{tv}$ irrespective of the amount spent on e.g. `radio`. \n\nThis assumption may be wrong and therefore the specified model may be incorrect.  For example, the amount spent on `radio` may actually  increase the effectiveness of `TV` advertisment. \n\nThis synergy (or dysynergy) effect is referred to as **interaction effect** in statistics.  \n\nOne way of incorporating this interaction effect into a regression model is to combine variables multiplicatively such that \n\n$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2 + \\epsilon$$\n\nThis can be rearranged such that:\n\n\\begin{equation}\n\\begin{split}\nY &= \\beta_0 + (\\beta_1+\\beta_3X_2)X_1 + \\beta_2X_2 + \\epsilon \\\\ \n& =  \\beta_0 + \\tilde{\\beta}X_1 + \\beta_2X_2 + \\epsilon\n\\end{split}\n\\end{equation}\n\nIn this case $\\tilde{\\beta}$ changes with $X_2$. Therefore, the effect of $X_1$ on $Y$ is no longer constant given that adjusting $X_2$ changes $X_1$ and $Y$.\n\nLet's look at our dataset and include the interaction effect of `TV`x`radio`. \n\nWe can see that the inclusion of the effedt increased $R^2_\\text{adj}$ significantly. Also the coefficient `TV:radio` ($\\beta_3$) is highly significant. \n\nInterpretation of $\\beta_3$: increase in effectiveness of TV advertisment on radio advertisment (or the other way around).\n\n## Dealing with qualitative data\n\nUp until now we have only worked with quantitative data, i.e. all variables in our models where numerical. However, this is not always the case. Explanatory variables can, of course, be qualitative. \n\nFor example, in our cars dataset we have a variable `origin`. The variable stores the information from which region the car is (US = 1, Europe = 2, Asia = 3).\n\nWhile the data is already encoded to be numerical the information it stores is still qualitative. For example, we cannot compare the data similar to numerial data. There is no numerical order and the distances between the values do not mean anything, i.e. 3 is not bigger than 1 in this case or a statement such as Asia is two more than US does not make sense. \n\n**How do we deal with this kind of data in our regression model?**\n\nIf we deal with two categories (male/female) we can simply create a *dummy variable* (0/1) of the form:\n\n\\begin{equation}\n  x_i =\\begin{cases}\n    1, & \\text{if $i$th person is female}\\\\\n    0, & \\text{if $i$th person is male}\n  \\end{cases}\n\\end{equation}\n\nThis results in the model:\n\n\\begin{equation}\n  y_i = \\beta_0 + \\beta_1x_i + \\epsilon =\\begin{cases}\n    \\beta_0 + \\beta_1 + \\epsilon, & \\text{if $i$th person is female}\\\\\n    \\beta_0 + \\epsilon & \\text{if $i$th person is male}\n  \\end{cases}\n\\end{equation}\n\n\nIf we deal with more than two categories we need to create additional *dummy variables*. For example in our case we need two variables of the form:\n\n\\begin{equation}\n  x_{i1} =\\begin{cases}\n    1, & \\text{if $i$th car is from US}\\\\\n    0, & \\text{if $i$th car is not from US}\n  \\end{cases}\n\\end{equation}\n\nand\\begin{equation}\n  x_{i2} =\\begin{cases}\n    1, & \\text{if $i$th car is from Asia}\\\\\n    0, & \\text{if $i$th car is not from Asia}\n  \\end{cases}\n\\end{equation}\n\nIf we then use both variables in our model we can describe it as:\n\n\\begin{equation}\n  y_i = \\beta_0 + \\beta_1x_{i2} + \\beta_2x_{i2} + \\epsilon =\\begin{cases}\n    \\beta_0 + \\beta_1 + \\epsilon, & \\text{if $i$th car is from US }\\\\\n    \\beta_0 + \\beta_2 + \\epsilon & \\text{if $i$th car is from Asia}\\\\\n        \\beta_0  + \\epsilon & \\text{if $i$th car is from Europe}\n  \\end{cases}\n\\end{equation}\n\nLet's look at an example using our car dataset. \n\nIf we treat the variable `origin` just as a regular numerial value, the regression model will not yield sensical results. \n\n\n\nIt does not make sense to say that asian cars (encoded as 3) have an impact of $3\\times5.4765$ on mpg, whereas as US cars only have an impact of $1\\times5.4765$\n\nHowever, when encoding the variable as *dummy variable* the model itself is correctly specified and the interpretation is intuitive. \n\n**How do we interpret the model results?** \n\nThe model yields two coefficients (besides the intercept):\n\n- coefficient for `origin=2` is $7.57$\n- coefficient for `origin=3` is $10.42$\n\nThey can be interpreted as follows. \n\n- the fact that the car is European (ceteris paribus) increases mpg by $7.57$\n- the fact that the car is Asian (ceteris paribus) increases mpg by $10.42$\n- the US case is not modelled explicitly but we can deduct that the mpg for US cars is $7.57$ lower compared to European cars and $10.42$ lower compared to Asian cars (on average)\n\n\n\n**Creating dummy variables**\n\nWe have created dummy variables using `C(origin)` (C stands for categorical data). We can also do this (i) manually or (ii) using pandas.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"S05a_Regression.html"},"language":{},"metadata":{"lang":"de","fig-responsive":true,"quarto-version":"1.2.269","theme":"cosmo","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}}}