{"title":"Clusteranalyse","markdown":{"headingText":"Clusteranalyse","containsRefs":false,"markdown":"\n\n# Clustering\n\n## Introduction\n\nClustering analysis refers to a rather broad set of techniques for __finding subgroups__ (i.e. clusters) in a data set. It has many applications in various researchs fields including business and economics. \n\nIn many fields the question if observations (e.g. companies, people, product, customers, stocks etc.) is worthwhile analysing in depth. \n\nGoal of cluster analysis is form subgroups based on __similarity of observations__. An ideal clustering result can be described as identifying subgroups that\n\n- are homogeneous within the subgroup, i.e. observations within a cluster are similar\n- are heterogenous between subgroups, i.e. observations of different clusters are not similar\n\nClustering analysis summarizes a set of approaches which differ in their approaches of:\n\n1. defining similarity (i.e. how do we define homogeneous observations?)\n\n2. forming groups (i.e. how do we group observations)\n\nAll clustering algorithms use all information (i.e. variables) for their analysis. \n\nGood applications / examples for the use of clustering analysis could be:\n\n- segmenting customers with respect to various factors into preference groups \n\n- segmenting stocks into different portfolios \n\n- segmenting companies into degree of innovation capacity\n\n\nIn machine learning and data science clustering analysis is classified as an _unsupervised learning_ technique, because it describes a set of algorithms which attempt to discover structure in a dataset. This is very different from our approaches we have used for _linear regression_ and _logistic regression_ which are _supervised learning_ algorithms. \n\n__What is differnt between _supervised_ and _unsupervised_ algorithms?__  \n\nIn our examples regession and classification cases we had access to a set of _J_ variables $X_1, X_2, \\ldots, X_J$ and a dependent variable $Y$ (both of which for $n$ observation). Our goal was to map $X$ on $Y$ this is we wanted to find models that used $X$ as an input to explain (or predict) $Y$. \n\nIn the _unsupervised learning_ approach we only have a set of variables $X_1, X_2, \\ldots, X_J$ (for $n$ observations). Here we cannot map $X$ on $Y$, i.e. we cannot and do not want to explain or predict $Y$. \nInstead we want to understand , e.g. if we can form subgroups among our observations. Unsupervised learning is often more challenging as there is no clear or simple goal to follow. \nThis makes it also more difficult to determine whether or not the end result is good. After all, unlike in the supervised case, we cannot compare our result to the true value (remember that in e.g. the regression or classification case we were able to compare our result $\\hat{y}$ to the true value $y$). To this end, unsupervised algorithms are often used as part of the _exploratory phase_ of statistical or quantitative analysis. \n\n\nIn clustering there are typically two different clustering approaches:\n\n    1. splitting approaches\n\n    2. Hierarchical clustering (agglomerative) approaches\n    \nIn this chapter we will focus on __K-Means clustering__ which is a form of splitting approach and only briefly illustrate hierarchical clustering and dendrograms. \n\nThis chapter is based on [James et al (2019), Chapter 10](http://faculty.marshall.usc.edu/gareth-james/ISL/). You can also find details regarding the agglomerative approaches there. \n\n## K-Means clustering\n\n__Goal of clustering__ is to separate a data set in to $K$ heterogenous and non-overlapping subgroups. K-means is a very simple, yet very effective way to achieve this. \n\nIn K-means clustering we need to specify the number of clusterss $K$ we require (that is where the name comes from). \n\n\nLet's have a look at an example from _James et al (2019)_ showing $150$ observations with $2$ variables. \n\n![Kmean-Example](https://www.dropbox.com/s/znd5nrqxnpobiab/Kmeans.png?dl=1)\n\n\nWe can see that  depending on the chosen $K$ the number of clusters identified by the algorithm is exactly $K$ (note: different clusters are represented by colors). It is important to realize that the clusters (i.e. the labels $1, \\ldots, K$) are not used in the clustering, but instead are the direct output, i.e. the result from the clustering algorithm. \n\n__How does the _K-means clustering_ exactly work?__ \n\nIn K-means clustering the following conditions for the clusters $C_1, \\ldots, C_K$ must be satisfied:\n\n- each observation belongs to at least one of the $K$ clusters\n\n- clusters are non-overlapping, i.e. no observation belongs to more than one cluster\n\nA __good cluster__ is then a cluster where the difference between each observation is very small (i.e. the _within-cluster variation_ is small). The K-Means clustering therfore needs to solve the following minimization problem:\n\n$$\\sum_{k=1}^K W(C_k) \\rightarrow Min$$\n\nwhere $C_k$ is cluster $k$ and $W(C_k)$ is measure for within-cluster variation, meaning that the sum of within-cluster variation needs to be as small as possible. \n\n### Euclidian distance\nIn order to solve this problem we need to define a measure of _within-cluster variation_. While there are various approaches to define _similarity_ the most common approach is defining similarity as the __euclidean distance__ which is defined as follows:\n\n$$d(p,q) = \\sqrt{\\sum_{n=1}^N (p_i - q_i)^2}$$\n\nwhere $p$ and $q$ are points or vectors. Let's look at an example to see what __Euclidean distance__ measures and how it is calculated.\n\nConsider the two points $p = (2, 3, 1)$ and $q = (4, 1, 2)$. Each point could represent one observation with three variables, for example. \n\nIf we want to calculate the distance between these two points we could use the measure of Euclidean distance and calculate the following:\n\n$$d(p,q) = \\sqrt{(2-4)^2 + (3 - 1)^2 + (1-2)^2} = 3$$\n\nThe euclidan distance therefore measure the squared difference for each coordinate of the point, takes the sum of all differences and then squares the result. \n\n> __EXCERCISE 1__: please work on the first exercise.\n\nIn clustering analysis we need to compare many euclidean distances with each other. It is therefore more convenient to omit the square root in the calculation. This can be achieved by squaring the euclidean distance. This is called the __squared Euclidean distance__. \n\nWith this we can now define the _within-cluster variation_ $W(C_k)$ more specifically:\n\n$$W(C_k) = \\frac{1}{n_k}\\sum_{i, i' \\in C_k} \\sum_{j=1}^{J}(x_{i,j} - x_{i'j})^2$$\n\nwhere\n- $n_k = $ number of observations in the cluster\n- $J = $ number of variables in the data set\n \nThe _within-cluster variation_ for cluster $k$ is therefore the sum of all of the pairwise squared Euclidean distances betwen all observations in cluster $k$, divide by the number of observations in cluster $k$. \n\nTo this end, the K-Means clustering algorithm needs to solve the following optimation problem:\n\n$$\\frac{1}{n_k}\\sum_{i, i' \\in C_k} \\sum_{j=1}^{J}(x_{i,j} - x_{i'j})^2 \\rightarrow Min$$\n\nIt turns out that this is a difficulat problem given there are almost $K^n$ ways divide $n$ observations into $K$ clusters. So assuming that we have 10 observations and 3 clusters there would exist $3^10 = 59.049$ possible cluster assignments. \n\nThe K-Means algorithm solves this problem in a much simpler ways (by finding a local instead of a global optimum). \n\n### Definition of K-Means algorithm\n\nThe K-Means algorithm is defined as follows:\n\n1. Initialize clusters by randomly assigning each observation a number from $1$ to $K$\n\n2. Calculate (new) cluster assignments until assignments do not change anymore. This is achieved by:\n\n    - compute centroids for each of the $K$ cluster. Centroid for cluster $k$ is the vector of means of each variable $j$ for all observations in cluster $k$ \n    \n    - compute euclidean distance between observations and centroids\n    \n    - assign each observation to the cluster where distance is the closest\n    \n    \nThe three steps under 2 are one iteration of the algorithm. This algorithm is repeated until the resulting clusters do not change anymore. \n    \n\n__INTERACTIVE EXAMPLE__ Let's first look at an interactive example to understand what the algorithm does. \n \n \n__Example__: assuming the following\n\n- we have a data set $X$ with $4$ observations and $2$ variables\n\n- we define $K$ to be 2 (i.e. we want the K-Means algorithm to find $2$ clusters)\n\nLet's go through each step of the algorithm manually:\n\n__Step 1:__ initialize cluster assignment\n\nOur random assignment yields the following:\n\n- observation $1$ and $4$ (i.e. rows $1$ and $4$ of X) are assigned to cluster $0$\n- observations $2$ and $3$ are assigned to cluster $1$\n\n__Step 2:__ calculate new cluster assignemnts\n\n__2a:__ calculate centroids for clusters\n\n__2b:__ calculate distance between observations and centroids\n\nLet's see if the results are reasonable by calculating the distance for our $1st$ observation to our $1st$ centroids manually:\n\n- $p = $observation $1$ = $(0.70 , 0.29)$\n- $q = $ centroid $1$ = $(0.84 , 0.485)$\n\n$$d(p,q)^2 = (0.70 - 0.84)^2 + (0.29 - 0.485)^2 \\approx 0.058$$  \n\nWe can confirm that the results are correct.\n\n__2c:__ assign observation to cluster where distance to centroid is the smallest. \n\n- observation $1$ $\\Rightarrow$ 0 (because 0.058 is smaller than 0.089\n- observation $2$ $\\Rightarrow$ 1 (because 0.376 is biggerer than 0.064)\n- observation $3$ $\\Rightarrow$ 0 (because 0.019 is smaller than 0.064)\n- observation $4$ $\\Rightarrow$ 0 (because 0.058 is smaller than 0.293)\n\nWe have gone through each step of the K-Means clustering algorithm. The above steps 2a to 2c are repeated until the cluster assignment does not change anymore. \n\n### K-Means clustering in Python\n\nWe can use a new module to use a readily available version of the __K-Means clustering__ algorithm. For this we need to import a new package called `sklearn` (see [here](https://scikit-learn.org/stable/)). In particular we need to import the clustering package `sklearn.cluster`.\n\nWe do this by doing\n\n> from sklearn.cluster import KMeans\n\nOnce we have done this we can\n\n1. instantiate a model defining the number of clusters $K$ \n\n2. fit the model to our data (i.e. calculate the clusters)\n\n\nLet's import the package and use it to see how it works:\n\n__Let's have a look at an example with more data__\n\n### Interpretation and evaluation of results\n\nOne of the key disadvantages with K-Means clustering (and clustering in general) is that there is no good way to decide if the model is good? This is do to the fact that there is no _true_ value to compare the results to. \n\nFuthermore there are some aspects that should be considered when using K-Means clustering algorithm:\n\n1. Results are somewhat depending on random initialization of clusters: it is therefore recommended to repeat the analysis several times to see if results change depending on random initialization. We should then use the __best__ model from this repetitions\n\n2. The best model is the model that has the lowest sum of average within-cluster deviations (i.e. the lowest value for our optimization function) for a constant $K$ (i.e. don't compare optimization results across different $Ks$).\n\n### [Appendix] Comparison Sklearn vs. manual implementation\n\nNote that the cluster number is random (arbitrary) and cannot be compared. For example, it could be that both implementations yield the same cluster groups but assign it different numbers (e.g. sklearn gives one cluster a $1$ whereas the manual algorithm assigns the same observations a $2$). \n\n## Hierarchical clustering\n\nThe following section will only briefly illustrate hierarchical clustering / dendrograms but will not cover the algorithm in detail. Please refer to [James et al (2019), Chapter 10, p. 392](http://faculty.marshall.usc.edu/gareth-james/ISL/) for further details. \n\nOne potential difficulty with K-Means clustering is that we don't know the correct number $K$ in advance. This is where _hierarchical clustering_ can be an helpful alternative as it does not require us to specify the number of clusters. \n\nHierarchical clustering results in in a tree-based representation of our observations: a __dendrogram__.\n\nLet's have a look at an example from [James et al (2019), Chapter 10, p. 392](http://faculty.marshall.usc.edu/gareth-james/ISL/).\n\nWe have a simulated data set with $45$ observations in a two-dimensional space, i.e. with two variables or features.\n\n![clusters](https://www.dropbox.com/s/bkrpjltv8fcf3xr/cluster_for_dendrogram.png?dl=1)\n\nThis can be represented by the following dendrogram.\n\n![dendrogram](https://www.dropbox.com/s/lc01ykbvtimvzrm/dendrogram.png?dl=1)\n\nIn a dendrogram can be interpreted as follows:\n\n- each leaf (i.e. the very bottom of the dendrogram represents one observation\n\n- observations that are similar to each other fuse into branches\n\n- the higher the branches fuse (on the y-axis) the further away the observations are (i.e. the lower the fusion occurs the more similar are the observations)\n\nThis interpretation is very helpful as it enables us to decide for ourselves how many clusters we believe are reasonabel. We can do this by drawing a horizontal line in the chart (see the middle and the right chart). The lower we draw this line, the more clusters we identify (extrem case: horizontal case is at the very bottom meaning each observation makes up one cluster).\n\nFor example: in the right chart the horizontal line is drawn at value of $5$ at the y-axis. This yields three clusters representing the three distinct sets of observations beneath the horizontal line. \n\n\nLet's look at another toy example to learn how to interpret these charts.\n\n__Example__\n\nSome observations from plotting the data set (giving each observation a number so we can identify it in the dendrogram):\n\n- observation 4 and 6 are close to each other\n- observation 5 and 7 are close to each other\n- observation 1 is further away from 4/6 and 5/7\n- obsevation 8 is even further away from 4/6 and 5/7\n\nLet's calculate a hierarchical clustering model and plot the corresponding dendrogram to see if these observations can be identified from the dendrogram. \n\nWhat can we see from the dendrogram that our observations are confirmed:\n\n- observations 5/7 and 4/6 are fusing at the very bottom of the chart indicating that they are similar to each other\n- observations 1 and 5/7 (4/6) are fusing higher up on the vertical axis than 5 and 7 (4 and 6)\n- observation 8 and 5/7 (4/6) are fusing even higher up on the vertial axis than 1 and 5/7 (4/6). \n\n__Advantages of hierarchical clustering__\n\n- we don't need to specify $K$\n\n- we can inspect dendrogram to decide on reasonable number of $K$\n\n\n__Disadvantage of hierarchical clustering__\n\n- interpretation can be quite difficult / confusing\n\n- choice of $K$ using hierarchical clustering can be arbitrary (where do we cut off?) \n\n## Considerations for clustering\n\nIn both approaches the algorithms _force_ each observation into a cluster. In K-Means even in the number of clusters $K$ we prespecify. This means irrespective of the _true_ grouping of the data (which unfortunately we don't know) we will always end up with each observation in a cluster. However, this may not be reasonable for the data set at hand because we may have - for example - outliers in our data or differences between (similar) observations that are not captured by our data set. \n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"S08a_Clustering.html"},"language":{},"metadata":{"lang":"de","fig-responsive":true,"quarto-version":"1.2.269","theme":"cosmo","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}}}