{"title":"Wahrscheinlichkeitsverteilungen","markdown":{"headingText":"Wahrscheinlichkeitsverteilungen","containsRefs":false,"markdown":"\n\nIn **probability** we assume a data generation process by which we \"create\" data. In short:\n\n- we know a model\n- we generate data using this model\n\nIn **statistical inference** we do the opposite. \n\n- we have data\n- we want to learn about the data generation process, i.e. what is the model?\n\nOften **quantitiatve research** deals with data with the goal to infer the data generation process / the model underlying the data. \n\n**Easy example**: coin flip.\n\n- Probability: we assume a random process that creates us coin flip data. We could, for example, assume it is a fair coin and \n\n- Inference: we have coin flip data and want to infer whether or not the coin is fair\n\n**More realistic examples**:\n\n- what are factors affecting customers to cancel a energy contract and switch to a competitor?\n\n- are women with equal qualifications more likely to receive less salary \n\nIt is, therefore, important that we have a good understanding of **probability** and **probability distributions** when conducting data analysis and infering from data. \n\nThe following section will therefore briefly repeat$^*$ several basic and important concepts. Once we have done so we will be able to:\n\n1. deal with uncertainties in our analysis / data \n\n2. interpret our analyses and models\n\n3. simulate data to support our data analysis\n\n($^*$it is again assumed that you have covered basic concepts of probability in a bachelor course)\n\n# Probability distributions\n\n## Definition probability\n\n(Note: this section is largely taken from Imai (2017))\n\n**Probability** is a measure of uncertainty. There are two basic ways to interpret probability:\n\n1. **Frequentist view**: probability represents the limit of the relative frequency with which an event of interest occurs when the number of experiments repeatedly conducted under the same conditions aproaches infinity.\n\n2. **Bayesian view**: probability is one's subjective belief about the likelihood an event occurs.\n\n\n\nThe definition of probability requires the following three concepts:\n\n1. **experiment**: an action or a set of actions that produce stochastic events\n\n2. **sample space**: a set of all possible outcomes of the experiment, typically denoted by $\\Omega$\n\n3. **event**: a subset of the sample space\n\nA **random variable** assigns a numeric value to each event of the experiment. These values represent mutually exclusive and exhausitve events, together forming the entire sample space. A **discrete random variable** takes a finite or at most countably infinite number of distinct values, whereas a **continuous random variable** assumes an uncountably infinite number of values. \n\nOnce we define a random variable, we can formalize a **probability model** using the distribution of the random variable. \n\nExamples of **discrete** random variables:\n\n| Experiment                       | Random variable ($x$)           | Possible values for random variable |\n|----------------------------------|---------------------------------|-------------------------------------|\n| Contact five customers           | # of customers placing an order | $0, 1, 2, 3, 4, 5$                    |\n| Inspect a shipment of 10 iPhones | # of defective devices     | $0, 1, 2, \\cdots, 9, 10$                   |\n| Operate a theme park for one day | # of customers             | $0, 1, 2,\\ldots$                        |\n| Sell a laptop                    | Gender of customers             | $0$ if female, $1$ if male              |\n\nExamples of **continous** random variables:\n\n| Experiment                  | Random variable ($x$)                             | Possible values for random variable |\n|-----------------------------|---------------------------------------------------|-------------------------------------|\n| Operate a McDonald's        | Time between customer arrivals in minutes         | $x \\geq 0$                          |\n| Fill a coffee mug           | Number of ml                                      | $0 \\leq x \\leq 500ml$               |\n| Construct a new library     | Percentage of project complete after six months   | $0 \\leq x \\leq 100$                 |\n| Test a new chemical process | Temperature when the desired reaction takes place | $-3 C^{\\circ} \\leq 120 C^{\\circ}$   |\n\n\nLet's look at some (selected) important distributions.\n\n## Characterization of probability distributions\n\nThere are two important functions which are helpful in characterizing probability distributions:\n\n1. Probability mass / density function\n\n3. Cumulative distribution function\n\n### Probablity mass / density function\n\nProbablity mass funtion (**PMF**): a function that gives the probability that a discrete random variable is exactly equal to one value $x$. \n\nExample:\n\n![two dice](https://www.dropbox.com/s/cefh5nkkurvdj6o/Two_dice.png?dl=1)\n\nNote that the sum of all bars must equal to 1 given the probability of all events occuring must be equal to 1. \n\nProbability density function (**PDF**): a function that specifies the probability of a continous random variable falling within a particular range of values (given the probabiltiy of a continuous random variable is zero)\n\nExample:\n\n![normal_dist](https://www.dropbox.com/s/7dnwhb1m03w7spo/Normal_distribution.png?dl=1)\n\nIn summary, probability mass / density functions are:\n\n- defined for any outcome of an experiment ($x$); (or range of $x$)\n\n- assign a probability to every possible $y$ \n\n- a function $f(x)$ such that:\n\n    - $f(x) \\geq 0$ for any $x$ \n    \n    - discrete: $x$: $\\sum_{\\text{all x}} f(x) = 1$;(sum of the heights is equal to 1)\n    \n    - continous: $\\int_{-\\infty}^{\\infty} f(x)dx = 1$; (area under the curve is equal to 1)\n\n### Cumulative distribution function\n\nUnlike the PMF the CDF gives you the probability that a random variable less than or equal $x$. \n\nFor a **discrete** distribution the CDF can be written as:\n\n$$F(x) = \\sum_{i=0}^x f(i)$$\n\nfor a discrete distribution the CDF is a step function where the function is flat and then jumpys ate each nonnegative integer value.\n\nHere is an example of the cumulative distribution function of the binomial distribution with $n=5$ and $p=0.5$\n\n\n![cdf_binomial](https://www.dropbox.com/s/0o2qdq9yax9vesw/Binomial_cdf.png?dl=1)\n\nThe horizontal axis is the allowable domain for the given probability function. Given the vertical axis is a probability, it must fall between 0 and 1. It increases from zero to one as we go from left to right on the horizontal axis.\n\n\nFor a **continous** distribution the CDF can be expressed as:\n\n$$F(x) = \\int_{-\\infty}^{x} f(t)dt$$\n\nHere is an example of the cumulative distribution function of the normal distribution with $\\mu=0$ and $\\sigma=1$:\n\n![cdf_normal](https://www.dropbox.com/s/4ccp1rpm4bf4cpe/Normal_cdf.png?dl=1)\n\n## Selected distributions\n\n### Binomial distribution\n\nBinomial outcomes are important to model, since they represent, inter alia, fundamental decisions such as stay or switch, buy or don't buy\n\n\nWith a binomial distribution we mean a random variable $X$ where:\n\n$X$ = The number of outcomes $y$ in $n$ binomial experiment\n\nRecall that a **binominal experiment** exhibits the following properties:\n\n1. The experiment consists of a sequence of $n$ identical trials\n2. Two outcomes are possible on each trial (success, failure)\n3. The probablity of a success, denoted by $p$, does not change from trial to trial (also the probability of failure - $1-p$ - does not change)\n4. The trials are independent\n\n\n#### Probability mass function\n\nThe *probabilty mass function* of a binomial random variable $x$ with probability $p$ and $n$ trials is given by:\n\n$$f(x) = P(X = x) = \\binom{n}{x}p^x(1-p)^{n-x}$$\n\n\nRecall that the binominal coefficient is defined as: $$\\binom{n}{x} = \\frac{n!}{x!(n-x)!}$$\n\nExample: assume we flip a coin 5 times. How many different combinations are there to get 2 heads out of 5 flips?\n\n- HHTTT\n- HTHTT\n- HTTHT\n- HTTTH\n- THHTT\n- THTHT\n- THTTH\n- TTHHT\n- TTHTH\n- TTTHH\n\nThere are $10$ unique ways to receive 2 heads out of 5 flips. This can be calculated by:\n\n\\begin{equation}\n    \\binom{5}{2} = \\frac{5!}{2!(5-2)!} = 10\n\\end{equation}\n\n**Example**: Binomial distribution\n\nWe are a producer of lawn mow roboters. In the next month we are required to produce $100$ robots. Over the last couple of years the defect rate was 5%.\n\n\n**Question:** What is the probability of producing \n\n1. **$0$** defective robots next month?\n\n2. **$100$** defective robots next month?\n\n3. **$5$** defective robots next month?\n\n**Probablity of $0$ or $100$ defects**\n\nFor the edge cases $0$ and $100$ it is easy to come up with the solution as these are single events that either occur or do not occur. \n\n**Probablity of exactly $5$ defects**\n\nThere are many combinations how 5 defects could occur. It could be the first 5 robots or the last 5 robots or many other any other combination of 5 robots out of 100. Therefore, we need the binomial coefficient $\\binom{n}{x}$. \n\nIn Python this can be done as follows:\n\nWe can easily put the definition of the probability mass function into a python function:\n\nThe probability that 5 out of 100 robots produced are defective is $~18\\%$. \n\n---\n\nLet's see what the probability mass function for all possible outcomes looks like for our experiment. \n\nWe can use this function to calculate the probabilities for all other possible events.\n\n#### Cumulative distribution function\n\nThe cumulative distribution function of a binomial random variable $x$ with probability $p$ and $n$ trials can be written as:\n\n$$F(x) = P(X \\leq x) = \\sum_{k=0}^{x}\\binom{n}{k}p^k(1 - p)^{n-k}$$\n\n\n\nAs you can see this is the sum of the PMFs. \n\n**Example**: \n\nWe are a producer of lawn mow roboters. In the next month we are required to produce $100$ robots. Over the last couple of years the defect rate was 5%.\n\n\n**Question:** What is the probability of producing \n\n1. less than $5$ defective robots next month?\n\n\n----\n\nLet's plot the CDF for our problem:\n\nWe can easily do this by taking the cumulative sum of the probabilities (use `np.cumsum`)\n\n#### Mean and variance of binomial distribution\n\nHow many defective parts do we expect when producing $100$ lawnn robots?\n\nFor one production unit we do expect $5\\%$ defective part; in $n = 100$ production units we, therefore, expect $n\\times0.05=5$ defective parts. \n\nThe **mean** of a binomial distribution is, therefore,  defined as:\n\n$$E(x) = n p$$\n\nThe **variance** of a binomical distribution is defined as:\n\n$$np(1-p)$$\n\nLet's calculate the mean and variance for our example:\n\n#### Using `scipy.stats` for probability distributions\n\nSo far we have implemented both pmf and cdf from scratch to illustrate the key idea of \n\n- the binomial distribution and\n\n- the ways how to implement these ideas directly into python\n\nFrom now on we will use existing modules to help us with probablity distribution (and simulation).\n\nUsing `scipy.stats` module it is even easier to calculate the key characteristics of probability distributions\n\nWe only need to:\n\n1. indicate which probability distribution we need\n\n2. call the `pmf` or `cdf` function or\n\n3. call `mean` and `var`/`std` functions\n\n\n\nLet's recalculate our lawn robot example using `scipy.stats`\n\nWhat is the probability of\n\n- exactly $5$ defects\n\n- less than $5$ defects\n\nWe can apply `pmf` and `cdf` on multiple x if we want to\n\nCalculating mean and variance of our binomial distribution is easy:\n\n\n### Poisson distribution\n\nAnother example for a discrete probability distribution is the **poisson probability distribution**. \n\nIt is very useful when describing or estimating number of occurences over a specified interval of time or space. \n\nExamples:\n\n- visitors viewing a website during one hour (over time)\n- cars arriving at a car wash in a day (over time)\n- the number of leaks in 250km of pipeline (over space)\n\nIf the following two properties are satisfied \n\n1. the probability of occurence is the same for any two intervals of equal length\n\n2. the occurence or non-occurence in any interval is independent of the occurence or non-occurence in any other interval\n\nthe number of occurences is a random variable which can be described by a **poisson probability distribution**.\n\n#### Probability mass function\n\nThe probability mass function (PMF) can be defined as follows:\n\n$$P(X = k) = \\frac{\\lambda^ke^{-\\lambda}}{k!}$$\n\nwhere:  \n$e = $ Euler's number (i.e. $2.71828\\ldots$ )  \n$k = $ the number of occurences  \n$\\lambda = $ the expected value (or mean) number of occurences in an interval\n\n\n\n**Example**: consider a coffee shop in the morning. In order to set up our staffing, we are interested in the number of customers arriving during an $5$ minutes period in the morning. Let's assume that the number of customers arriving at our coffee shop follows a poisson process, i.e. \n\n- we assume that the probability of a customer arriving is the same for any two time intervals of equal length (during that $5$ minutes)\n\n- we assume that the arrival of customers during the $5$ minute period is independent of each other. \n\nFrom previous experience we know that average number of customers arriving during that $5$ minute time period is $7$. \n\nWe can now calculate the probability of $k$ customers arriving as follows:\n\n$$P(X=k) = \\frac{7^ke^{-7}}{k!}$$\n\nE.g. the probability of only $4$ customers arriving in $5$ minutes is $P(4) = \\frac{7^{4}e^{-7}}{4!} \\approx 9.12\\%$\n\nUsing `scipy.stats` we can now calculate the entire probability mass function, easily. \n\n#### Cumulative distribution function\n\nAs with the binomial distribution there is no distinct expression for the cumulative poisson distribution. Instead is also given simply be the sum of the probability mass functions:\n\n$$P(X \\leq k) = \\sum_{x=0}^k \\frac{\\lambda^ke^{-\\lambda}}{k!}$$\n\n**Example**: the probability for 0 to 3 (inculding) customers arriving is $P(X=0) + P(X=1) + P(X=2) + P(X=3) \\approx 8.18\\%$\n\nThe CDF of a Poission distribution is then also a step function. \n\n#### Mean and variance of distribution\n\nThe mean and the variance of the distribution are both given by\n\n$$E(X) = Var(X) = \\lambda$$\n\n### Normal distribution\n\nArguably one of the most important and known examples of a continous distribution is the *normal distribution* (often also called the *gaussian normal distribution*). It can be considered a *special* distribution because, as it's name implies, the sum of random variables from a given (non-normal) distribution tend to follow the normal distribtion.\n\nThe normal distribution has many practical applications as many random variables are following normal distributions, e.g.:\n\n- heights, weights and IQ of people\n- students test scores\n- amounts of rainfall\n- stock returns (although with fat tails)\n\nIt is also an important probability distribution for many aspects in statistical inference. \n\nA normal random variable can take any number on the real line $(-\\infty, \\infty)$. Note that the probability for any single value from a continuous distribution is zero. This is because the probability of getting a value of exactly 3.45 in on the real line $(-\\infty, \\infty)$ is zero. Therefore think of a value $x$ an infinitesimal small range around $x$. \n\n#### Probability density function\n\nThe probability density function (pdf) is defined as follows:\n\n$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n\nwhere:\n\n$\\mu =$ is the mean of the distribution  \n$\\sigma =$ the standard deviation of the distribution\n\nWe can model these distributions using `scipy.stats.norm`. Calculating the densities (not probabilities) of $x$ can then be calcualted using `pdf` (not pmf!).\n\nHere are three different normal distributions with different means and standard deviations\n\n**Example**: let's say the height of humans can be described as a normal random variable (in cm).\n\n- height of males can be described as a random normal variable with $\\mu =  170$ and $\\sigma = 20$ \n\n- height of women can be described as  random normal variable with $\\mu =  160$ and $\\sigma = 18$ \n\n#### Cumulative distribution function\n\nThe cumulative probability distribution (CDF) has no analytically tractable form is given by:\n\n$$F(x) = P(X \\leq x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} dt$$\n\nIt represents the area under the PDF from $-\\infty$ up to $x$.\n\n**Let's continue our example:**\n\n- what is the probability of women being smaller or equal  to 1,40m?\n\n- what is the probability of men being larger than 2m?\n\nWe can plot the cumulative probability distribution by\n\n## Approximation of distributions\n\nIt is important to note that many distributions can be approximated with different distributions if certain properties hold. \n\nWe will look at the following three approximations:\n\n- binomial distribution approximated by normal distribution\n- binomial distribution approximated by poisson distribution\n- poisson distribution approximated by normal distribution\n\n### Approximation: binomial distribution through normal distribution\n\nIf the following condition holds:\n\n- $n$ is sufficiently large\n- $p$ not to close to the edges $0$ or $1$\n\nthan a binomial distribution can be approximated by a normal distribution, where:\n\n- $\\mu = np$\n- $\\sigma = \\sqrt{np(1-p)}$\n\n**Example**\n\n### Approximation: binomial distribution through poisson distribution\n\nIf the following condition holds:\n\n- $n$ is sufficiently large\n- $p$ is sufficiently small\n\nthan a binomial distribution can be approximated by a poisson distribution, where:\n\n- $\\lambda = np$\n\n**Example**\n\n### Approximation: poisson distribution through normal distribution\n\nIf the following condition holds:\n\n- $\\lambda$ is sufficiently large\n\n\nthan a poisson distribution can be approximated by a normal distribution, where:\n\n- $\\mu = \\lambda$\n- $\\sigma = \\sqrt{\\lambda}$\n\n## Probability distributions and simulations\n\nWe have previously discussed and seen the benefits of simulation. We had stated the following (selected) use cases for simulation:\n\n1. solve probability problems\n\n2. calculate features of probability densities\n\n3. understand the assumed data generation process\n\n4. evaluating estimators\n\nWe can draw random samples from a distribution and then investigate that sample to learn something about the features of a distribution. Also we can use these random samples to simulate real world problems. \n\nUsing `scipy.stats` we can define distributions and than use `rvs` to draw random samples from this distribution. \n\nExample:\n\n### Creating synthetic data\n\nWe can use this to generate (synthetic) data sets to test our models on. \n\nExample: let's create a linear data set.\n\n$$y = \\beta_0 + \\beta_1x + \\epsilon$$\n\nIf we synthetically create this data we can run regression models on this data and know the ground truth. This is very helpful when trying to understand the specifics of a model and to see which modells work well on which kind of data.\n\nLet's make the following assumptions:\n\n- $x$ is a random poisson variable with $\\lambda = 5$.\n- $\\beta_0=10$ and $\\beta_1 = 4.5$\n- $\\epsilon$ is N(0,1) by definition\n\nWe can see that if our research data is expected to be distributed as defined a simple linear regression model would work really well. \n\n### [ADVANCED] Simulating real world problems\n\nSee [here](https://nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Economics.ipynb) for a great example (which also produces a counterintutive result).\n\nA minor extract from that example is reproduced (in a slightly different fashion) below. \n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"S03c_Probability_Distributions.html"},"language":{},"metadata":{"lang":"de","fig-responsive":true,"quarto-version":"1.2.269","theme":"cosmo","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}}}