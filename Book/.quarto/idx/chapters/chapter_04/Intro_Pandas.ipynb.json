{"title":"Pandas","markdown":{"headingText":"Pandas","containsRefs":false,"markdown":"\n\nWir stellen nun das für uns wohl wichtigste Modul in Python vor: [`Pandas`](https://pandas.pydata.org/getting_started.html). Pandas baut \"im Hintergrund\" auf dem im vorherigen Kapitel vorgestellten Modul `numpy` auf, ist jedoch für einen etwas anderen Zweck erstellt worden. \n\nIm Kontext der Datenanalyse haben wir es oft mit tabularischen Daten zu tun, d.h. wir arbeiten mit Informationen und/oder Daten, die in Tabellen gespeichert sind. Typischerweise haben diese Informationen Beschriftungen und sind von unterschiedlichen Typen. Stellen wir uns dies in Excel vor: wir haben mehrere Spalten, jede Spalte hat eine Bezeichnung, die beschreibt, um welche Information es sich handelt. Einige Spalten haben numerische Informationen (z.B. Umsätze), andere haben text-basierte Informationen (z.B. Namen oder Orte), wieder andere haben ein Datum als Spalteninhalt. Pandas ist das Modul, welches für den Umgang mit dieser Art von Daten gemacht wurde. \n\n\nUnseren bisherigen Beispiele basierten oft auf vereinfachenden Datensätzen, die nicht vergleichbar mit denen sind, die wir in Unternehmen typischerweise vorfinden. Diese Beispiele dienten zur Veranschaulichung der Grundlagen der (datengetriebenen) Programmierung und sind für die eigentliche Datenanalyse wie wir sie durchführen wollen nicht realistisch. Natürlich hat dies didaktische Gründe. \n\nWenn wir es mit typischen Fragestellungen im unternehmerischen Kontext (aber auch in der Forschung) zu tun haben, dann besteht ein Analyseprozess jedoch oft auf verschiedenen Schritten:\n\n1. Einlesen von Daten\n\n2. Daten aufbereiten und ggf. ergänzen\n\n3. Daten explorativ untersuchen und zusammenfassen\n\n4. Daten analysieren  \n\n5. Daten präsentieren in Form von Tabellen oder Graphiken\n\nPandas unterstützt alle diese Schritte, in dem es hierfür sinnvolle Funktionalitäten bereitstellt. Die gesamte Bandbreite an Funktionalitäten ist so groß, dass es unmöglich ist, diese im Rahmen _eines_ Kurses zu erläutern und vorzustellen. Zusätzlich ist es so, dass Pandas am Besten im Zusammenspiel mit anderen Modulen (z.B. Numpy, aber auch Modulen zur Visualisierung) eingesetzt wird. Wir werden uns in diesem Kapitel auf selektive, aber - aus unserer Perspektive - wesentliche Funktionalitäten beschränken (müssen). Insbesondere werden wir uns darüber hinaus auch auf Techniken konzentrieren, die für den Einsatz von Pandas besonders geeignet sind. \n\n```{admonition} Nützliche Ressourcen zu Pandas\n:class: tip, dropdown\n\nEs gibt im Internet viele frei verfügbare Ressourcen zu Pandas. Eine sehr gelungene Einführung ist das Kapitel aus [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html). Außerdem ist das [Cheat-Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) mit einer Zusammenfassung wichtiger Methoden sehr hilfreich. \n\n```\n\nIn diesem Kapitel werden wir einige wichtige Grundlagen von Pandas vorstellen. Wir nutzen hierfür - wie in den vorherigen Kapiteln auch - kleine Beispieldaten. Im nächsten Kapitel werden wir dann realistischere Datensätze verwenden, um wichtige Konzepte quasi am \"lebenden Objekt\" zu erlernen. Wir glauben, dass Sie zunächst einige wichtige Grundlagen verstanden haben müssen. Um diese zu erläutern benötigen wir keine \"echten\" Daten. Ab einem gewissen Punkt geht es jedoch nicht mehr darum isolierte Funktionen vorzustellen, sondern vielmehr den tatsächlichen Prozess der Datenanalyse zu durchlaufen, um dann auf reale Probleme zu stoßen und praxirelevante Lösungsansätze vorzustellen. \n\nBeginnen wir mit den Grundlagen.\n\n## Grundlagen\n\nWir stellen im Folgenden ausgewählte Grundlagen des Moduls vor. Hierbei fokussieren wir uns auf \n\n1. das Importieren des Moduls \n2. das Erstellen von `Series` und `Dataframes`\n\n### Importieren des Moduls\n\nBevor wir Pandas nutzen können, müssen wir das Modul zunächst importieren. Wir können hier prinzipiell wieder vorgehen, wie wir dies im Kapitel [Einführung: Module](../chapter_04/Intro_Modules.ipynb) ausgeführt haben. Typischerweise wird Pandas jedoch - analog zu Numpy - als ganzes Modul importiert und dann mit einer Kurzbezeichnung versehen. Es ist daher empfehlenswert, Pandas wie unten aufgeführt zu importieren.  \n\n### Series und Dataframes\n\nDas Herzstück von Pandas ist der sogenannte Dataframe (`pd.Dataframe`). Sie können sich ein Dataframe als eine Art Excel-Sheet bzw. eine Excel-Tabelle vorstellen. So wie in Excel jede Tabelle aus einer Ansammlung aus Spalten besteht, ist ein Dataframe in Pandas eine Ansammlung von Series (`pd.Series`).\n\nSchauen wir uns das untenstehende Excelbeispiel an.\n\n![xlsdataframe](../assets/dataframe_xls.png)\n\nDie abgebildete Excel-Tabelle ist in dieser Analogie ein `dataframe`. Jeder ihrer Spalten eine `series`. Die Spalte \"Preis\" wäre eine Series mit `floats`, d.h. mit Dezimalzahlen. Die Spalte \"Datum\" wäre eine Series vom Typ `str` oder von einem Typ, der ein Datum abbilden kann (diesen Datentyp werden wir in diesem Kapitel noch kennenlernen).  \n\nSchauen wir uns an, wie wir diese Daten in `pandas` abbilden. \n\n## Erstellen von Series\n\nWir können - ähnlich wie beim Numpy Array - Series relativ einfach manuell erstellen. Wenn wir die Spalte \"Menge\" nachbilden möchten, so können wir z.B. folgendes schreiben:\n\nWir sehen, dass das Vorgehen fast identisch ist zur Erstellung eines Numpy-Arrays. Ein wichtiger Unterschied ist jedoch, dass wir der Series einen Namen geben können. Wir haben hier die Namen \"Menge\" vergeben. Wir können uns den Namen wie die Spaltenbezeichnung in einer Exceltabelle vorstellen. Die in der Variable \"s1\" gespeicherte Series hat den Namen \"Menge\" und ist vom Typ `int`[^1]. \n\n[^1]: Hinweis: genauer gesagt vom Typ `int64`. Die Zahl hinter `int` macht deutlich, dass es eine 64-bit Integer ist und gibt uns damit Aufschluss über den abbildbaren Wertebereich (und damit auch über den benötigten Speicher) des Datentyps. Diese technischen Feinheiten sind für unsere Zwecke an dieser Stelle nicht relevant. Weitere Informationen dazu finden Sie z.B. [hier](https://de.wikipedia.org/wiki/Integer_(Datentyp)).\n\n\nErstellen wir nun eine Series, die die Spalte \"Preis\" abbildet und speichern diese in der Variable \"s2\". Wir schreiben den ersten Wert mit Dezimalpunkt, da die Series dann automatisch erkennt, dass es sich um den Datentyp `float` handelt. \n\nWir könnten den Datentyp auch explizit angeben. Der untenstehende Code gibt z.B. an, dass die Series vom Datentyp `str` sein soll, so dass die Elemente Texte sind. In Pandas wird dies dann als `object` bezeichnet. \n\n## Erstellen von Dataframes\n\nDataframes können manuell erstellt werden oder aus externen und einzulesenden Daten erstellt werden. Wir gehen hier zunächst auf die manuelle Erstellung ein. Diese benötigen wir in der Praxis oft gar nicht, weil wir meist auf externe Daten (z.B. Excel-, CSV-, SQL- oder TXT-Dateien) zugreifen. Dennoch ist es wichtig zu verstehen, wie Dataframes manuell erstellt werden können, da dies (i) hilft, den grundsätzlichen Aufbau von Dataframes zu verstehen und (ii) oft nützlich ist, um kleinere Datensätze zu Testzwecken zu erzeugen. \n\n### Manuelle Erstellung\n\nWir können Dataframes auf verschiedene Art und Weisen erstellen. Wir gehen hier auf eine Variante ein, die aus unserer Sicht oft genuttz wird und die aller meisten Anwendungsfälle abdeckt: wir erstellen Dataframes über Dictionaries (`dict`). Den Datentyp haben wir im Kapitel **Einführung in Python** [hier](intro:dicts) bereits kennengelernt.\n\nWir erinnern uns, dass ein Dictionary jeweils wiefolgt aufgebaut ist:\n\n```\n{key: values}\n```\n\nWenn wir ein Dataframe mittels Dictionary erzeugen, dann werden die `Keys` als Spaltenbezeichnung übernommen und die `Values` dann als Daten unterhalb der Spaltenbezeichnung eingelesen. \n\nHier der Beispielcode für ein Dataframe, welcher die Spalten (in dem Falle Series) **\"Preis\"** und **\"Menge\"** beinhaltet.\n\nWir sehen, dass die `Keys` \"Menge\" und \"Preis\" als Spaltenbezeichnung übernommen wurden. Die einzelnen Werte sind im obigen Falle Listen gewesen. \n\nWir können hier jedoch auch andere Datentypen nutzen. So können wir auch `Series` oder `Arrays` einfügen. Dies ist sehr praktisch, da wir im Rahmen unser Analysen mit verschiedenen Datentypen arbeiten werden. \n\n**Beispiel: Series**\n\n**Beispiel: Arrays**\n\n### Einlesen von Daten\n\nWie bereits angesprochen ist es realistisch, dass wir bereits über Daten verfügen und diese in Pandas einlesen möchten. Hierfür bietet Pandas eine Vielzahl an Methoden, die es ermöglicht nahezu jeden Dateityp einzulesen. Wir werden uns in den nächsten Kapiteln auf das Einlesen von Excel- und CSV-Dateien beschränken. Das grundsätzliche Vorgehen ist jedoch - unabhängig vom Dateityp - sehr ähnlich. \n\nPandas bietet die Methoden zum Einlesen von externen Daten via `pd.read_` an. \n\nWir können uns über `pd.read_<tab>` anzeigen lassen, welche Funktionen zur Verfügung stehen und uns die gewünschte raussuchen. \n\n![pdread](../assets/pdread_tab.png)\n\nZum Einlesen der gewünschten Daten benötigen wir typischerweise den Dateipfad, d.h. den Ort, in welchem die Datei auf dem Computer oder in der Cloud hinterlegt ist. In unserem Falle sind die Daten in einer Cloud hinterlegt, d.h. wir geben einen Link zur Cloud-Storage an. \n\n\n````{admonition} Einlesen lokaler Dateien\n:class: note, dropdown\n\nFür den Fall, dass die Dateien lokal gespeichert sind, geben Sie einfach den absoluten oder relativen Pfad zur Datei ein. Haben wir bspw. folgende Struktur:\n\n```\n-- Ordner\n---- analyse.ipynb\n---- Ordner_Daten\n------ daten.xlsx\n```\n\nDann können Sie die Datei `daten.xlsx` in ihrem Notebook `analyse.ipynb` wie folgt einlesen:\n\n```\nimport pandas as pd \n\nfpath = \"Ordner_Daten/daten.xlsx\n\npd.read_excel(fpath)\n\n```\n\nBei dem o.g. Pfad handelt es sich um einen `relativen` Ordnerpfad, weil dieser relativ zu Datei in der sie sich befinden ist (in diesem Falle `analyse.ipynb`). Natürlich können Sie auch den gesamten Pfad angeben also z.B. `c:\\user\\analysen\\Ordner\\Ordner_Daten\\daten.xlsx`.\n\n````\n\n\n\nDie jeweiligen `pd.read_`-Methoden haben eine Vielzahl von optionalen Parametern, die man übergeben kann. Wir werden an dieser Stelle nicht auf alle eingehen. Grundsätzlich können Sie aber quasi davon ausgehen, dass es für alle typischen Optionen, die man beim Einlesen von Daten benötigt (z.B. nur bestimmte Spalten, nur bestimmte Tabellenblätter, Auswahl der Spaltenbezeichnung, Auswahl von Datentypen, Datumsformat) eine optionale Auswahlmöglichkeit innerhalb der Methode gibt. Sie können diese via `pd.read_<..>?` aufrufen lassen. \n\n## Aufbau eines Dataframes\n\nEin Dataframe ist immer eine Tabelle, mit zwei Achsen: \n\n- vertikale Achse: wird als `Index` bezeichnet\n- horizontale Achse: werden als `Columns` (d.h. Spalten) bezeichnet\n\nDer Index eines Dataframes ist standardmäßig numerisch und beginnt bei 0. Der Index ist vergleichbar mit den Zeilenbezeichnungen in Excel.\n\n![index_xls](../assets/index_xlsx.gif)\n\n\n\n\nWir greifen auf Index und Spalten wiefolgt zu:\n\nÜber diese Logik können wir auch neue Werte für den Index oder die Spaltenbezeichnung erstellen. \n\nLassen Sie uns die alten Bezeichnungen für unsere weiteren Beispiele jedoch wieder herstellen:\n\nSchauen wir uns nun weitere Funktionalitäten von Pandas an. \n\n## Auswahl von Daten\n\nIm Rahmen einer Datenanalyse wird es vorkommen, dass wir uns auf bestimmte Zeilen oder Spalten eines Dataframes beziehen wollen. Schauen wir uns an, wie wir dies in Python machen. \n\nLeider gibt es verschiedene Ansätze, um Daten zu selektieren. Wir werden hier ein paar gängige Ansätze vorstellen. Im Rahmen ihrer Online-Recherchen werden Sie ggf. auf weitere Ansätze stoßen. Hierbei gibt es oft kein \"richtig\" oder \"falsch\". Schauen Sie, was für Sie ein passender Ansatz ist. Wir geben darüber hinaus an den relevanten Stellen Hinweise, wenn bestimmte Ansätze sich als besonders geeignet herausstellen. \n\n### Auswahl von Spalten\n\nEin einfacher und gängiger Weg, bestimmte Spalten zu selektieren, ist die Auswahl via Spaltenname; dies ist ja auch der Vorteil gegenüber einer rein numerischen Bibliothek wie Numpy.  \n\nHier ein Beispiel für die Selektion einer Spalte. Das Resultat ist eine `Series`.\n\n````{admonition} Vorsicht: Spaltenauswahl\n:class: warning, dropdown\n\nSie werden in verschiedenen Quellen auch die folgende Variante finden\n\n```\nmenge = df.Menge\n```\n\nDer Ansatz ist grundsätzlich korrekt, jedoch nicht empfehlenswert. Hintergrund ist, dass dieser nicht funkioniert, wenn Spaltenbezeichnungen mit einer Zahl beginnen oder Leerzeichen beinhalten. Die Spalte `Produkt Nr.` ist so bspw. nicht selektierbar. Sie müssen also auf die von uns präferierte Variante zurückgreifen. Insofern bietet sich die von uns dargestellte Notation besser an, da diese immer funktioniert. \n\n\n````\n\nNatürlich können wir aber auch mehrere Spalten gleichzeitig auswählen. Wir machen dies, in dem wir eine Liste mit Spaltennamen übergeben. Das Resultat ist dann ein `Dataframe`. \n\nWir können die gleiche Funktionalität auch mit einer speziellen Funktion `.filter` erreichen. \n\nDie Funktion bietet weitere Möglichkeiten. So können wir Spalten auch nach bestimmten (einfachen) Logiken filtern. Wir könnten z.B. interessiert sein an allen Spalten, die mit \"P\" beginnen. Dies können wir einfach erreichen mit\n\n### Auswahl von Zeilen\n\nSie können auch spezifische Zeilen auswählen. Die üblichste Variante ist dies über die Methoden `iloc` und `loc` zu machen. \n\n- Auswahl auf Basis von Index- oder Spalten**numerierung**: `iloc` \n- Auswahl auf Basis von Index- oder Spalten**bezeichnung**: `loc` \n\n**Wichtig:** auch in Pandas ist das erste Elemente wie bei allen anderen Datentypen für die dies relevant war sowohl für Index, als auch für Columns immer an der Stelle `0`. \n\nGerade für Beginner ist die Auswahl via `iloc` und `loc` zunächst verwirrend. Lassen Sie uns deshalb einige typische Beispiele durchgehen. \n\nZur Übersicht stellen wir hier noch einmal unseren Ausgangsdatensatz dar:\n\n**Beispiel 1:** Auswahl der Zeilen 0, 3 und 5\n\n**Beispiel 2:** Auswahl der Zeilen 0 bis 3 (ausschließlich)\n\nSie werden feststellen, dass die grundsätzliche Logik sehr ähnlich zu der in Numpy ist. Dies ist auch kein Zufall, da Pandas auf Numpy aufsetzt und \"im Hintergrund\" Numpy nutzt. \n\nSchauen wir uns zwei weitere Beispiel an.\n\n**Beispiel 3:** Auswahl der Zeilen 0 bis 3 und der Spalten 0 und 2\n\n\n\n**Beispiel 4:** Auswahl der Zeilen 0 bis 3 und der Spalten 0 bis 3 (ausschließlich)\n\nDie oberen Beispiele haben wir `iloc` genutzt: wir greifen jeweils auf die Numerierung der Spalte oder des Index zu. D.h. wir greifen auf z.B. Zeile 2 oder Spalte 0 zu. \n\nSobald wir auf den Index oder die Spalte mit einer bestimmten Bezeichnung zugreifen wollen, dann müssen wir `loc` nutzen. Dies ist insbesondere im Falle der Spalten sehr häufig der Fall. \n\n**Beispiel 5:** Auswahl der Zeilen 2 bis 4 und der Spalten \"Preis\" und \"Menge\"\n\nIn vielen Anwendungsfällen wollen wir die Spalten über die Spaltenbezeichnungen auswählen, d.h. wir müssen `loc` nutzen. In weniger häufigen, aber durch aus teilweise relevanten Anwendungsfällen ist ggf. auch der Index nicht standardmäßig nummeriert. Auch dann müssen wir `loc` nutzen. \n\nHier ein Beispiel mit einem angepassten Index:\n\n**Beispiel 6:** Auswahl der Zeilen \"a\" und \"d\" und der Spalten \"Datum\" und \"Menge\"\n\nWie bei Numpy können wir Zeilen oder Spalten auch via `boolean`s auswählen. \n\nSchauen wir uns ein paar Beispiele an:\n\n**Beispiel 7:** Auswahl alle Zeilen, alle Spalten für die True\n\n**Beispiel 8:** Auswahl alle Zeilen für die True, alle Spalten für die True\n\nIn den beiden letzten Beispielen können wir `iloc` oder `loc` verwenden. Wir empfehlen jedoch `loc` zu nutzen, da dies im weiteren Verlauf noch hilfreich sein wird. Die Wahl hängt auch meist von der Art der gewählten Spaltenauswahl ab. \n\nIm folgenden Beispiel - ein häufiges Muster der Datenselektion, wie wir im weiteren Verlauf noch feststellen werden - müssen wir wegen der Spaltenauswahl über die Spaltenbezeichnung `loc` nutzen. \n\n**Beispiel 9:** Auswahl alle Zeilen für die True, Spalte \"Menge\" und \"Preis\"\n\nDas letzte Beispiel ist ein sehr typisches Beispiel im Rahmen einer Datenanalyse: wir filtern einen Datensatz hinsichtlich einer bestimmten logischen Bedingung und lassen uns dann die für uns relevaten Spalten anzeigen. Schauen wir uns dieses Vorgehen etwas detaillierter an. \n\n### Bedingungen und logische Auswahl\n\nIn den vorliegenden Beispielen haben wir eine explizite Auswahl getroffen und diese selbst definiert. In vielen Fällen werden wir jedoch eine Auswahl auf Basis von Bedingungen der logischen Verknüpfungen vornehmen wollen. Wir haben bereits im vorherigen Kapitel in Numpy gesehen, wie wir eine solche Auswahl treffen. In Pandas erfolgt die Vorgehensweise sehr ähnlich. \n\nStellen wir zunächst unseren Ausgangsdatensatz wieder her:\n\nStellen wir uns vor, wir wollen nur alle Datensätze anzeigen lassen, bei denen die `Menge > 5` ist. \n\nInhaltlich splittet sich diese Frage auf in zwei Teile:\n\n1. finde alle Zeilen, bei denen Menge > 5 \n2. wähle alle Zeilen aus, die unter 1 identifiziert wurden\n\nWir können den Schritt 2 in diesem Falle auch kürzer darstellen und folgendes schreiben:\n\nZusammenfassend kann man also kompakter schreiben:\n\nWenn wir jedoch nur auf einige bestimmte Spalten zugreifen wollen, sollten wir wieder `loc` nutzen, d.h. wir müssten dann z.B. folgendes schreiben\n\nWir erkennen also, dass die logische Bedingung im vorliegenden Fall die Zeilen definiert, die wir extrahieren wollen. \n\nWie bei Numpy können wir auch wieder verschiedene logische Bedingungen miteinander verknüpfen. Die Schreibweise ist dann sehr ähnlich zu der in Numpy. \n\nWenn wir bspw alle Transaktion (sprich Zeilen) finden wollen, für die gilt, dass die **Menge > 5** ist **und** der **Preis == 5**, dann schreiben wir:\n\noder kompakter:\n\nIm Falle von komplexeren Bedingungen bietet Pandas auch die Möglichkeit eine dafür spezielle Methode zu nutzen: `query`. Mit dieser Funktion können wir typische Bedingungen etwas prägnanter formulieren. \n\n## Operationen auf Daten\n\nEin typisches Muster der Datenanalyse ist, dass wir Rohdaten einlesen und diese dann korrigieren, verändern, ergänzen oder erweitern. Ein sehr triviales, aber häufig auftretendes Beispiel ist, dass wir unserem Datensatz Spalten hinzufügen:\n\n1. wir ergänzen Spalten auf Basis von vorliegenden Daten: wir könnten z.B. eine neue Spalte `Umsatz` berechnen auf Basis der vorliegenden Daten für `Menge` und `Preis`\n2. wir wenden mathematische Funktionen auf Daten an: wir erstellen eine neue Spalte mit dem kumulierten Umsatz\n3. ...\n\nSchauen wir uns an, wie wir die o.g. Veränderungen in Pandas durchführen. \n\n### Erzeugen von neuen Spalten\n\nEine neue Spalte erzeugen wir, in der wir quasi so tun, als ob diese existierte. Um z.B. eine neue Spalte `Umsatz` zu erzeugen machen wir folgendes:\n\nEine alternative Umsetzung - die sich in einem anderen Kontext im weiteren Verlauf noch als nützlich herausstellen wird - kann mittels der Methode `assign` erfolgen. Hier ein Beispiel, der eine neue Spalte `Umsatz_v2` erstellt: \n\nWir können mit numerischen Spaltendaten genauso wie bei Numpy ganz einfach rechnen. Pandas wendet die mathematische Operation - in unserem Fall `*` - dann elementweise auf die gesamte Spalte an. Der oben dargestellte Code ist das Äquivalent von folgendem Beispiel in Excel:\n\n![](../assets/add_column_xls.gif)\n\n### Anwenden von komplexeren Operationen\n\nWir können so auch wesentlich komplexere Berechnungen durchführen - unsere eigenen Ideen sind quasi das Limit. Für viele typische Berechnungen stellen Pandas und/oder Numpy auch bereits Methoden zur Verfügung. Wir werden an dieser Stelle nicht im Detail darauf eingehen, sondern viele der Funktionen einführen, wenn wir diese in unseren Fallstudien benötigen. \n\nJedoch möchten wir ein Beispiel geben. Stellen Sie sich vor, wir wollen die kumulative Summe der Menge berechnen und in einer Spalte `Menge_kum` abspeichern. Wir können dies dann mit der Funktion `cumsum` machen und müssen hierfür keine eigene Funktion schreiben.\n\nDie gleiche Berechnung können wir auch via Numpy durchführen. Wir würden dann die Numpy-Funktion `np.cumsum` nutzen. Dies würde dann wiefolgt aussehen:\n\nNatürlich können wir auch unsere eigene Funktion schreiben und diese dann auf unseren Dataframe anwenden. \n\nAuch wenn wir an dieser Stelle nicht weiter auf alle verfügbaren Informationen eingehen können. Die Beispiele zeigen, dass wir - sobald wir mathematische, statistische oder sonstige Funktionen auf unsere Daten anwenden wollen - im Hinterkopf haben sollten, dass wir sowohl Pandas, als auch Numpy dafür nutzen können. Falls keine Funktion existiert, können wir diese selber schreiben. Wir können unsere Dataframes demnach nach belieben verändern und ergänzen. \n\nNatürlich können wir aber auch einzelne Werte einer ganzen Spalte zuweisen. Stellen Sie sich vor, wir haben die Information, dass die vorliegenden Menge alle durch den Verkäufer \"Herr Müller\" verkauft wurden. Wir wollen diese Information ergänzen. Wir erstellen dann eine Spalte `Verkäufer`\n\n## Aggregationen von Daten\n\nWieder vergleichbar zu Numpy können wir Daten, die in einem Dataframe enthalten sind aggregieren und zusammenfassen. Jedoch ist es wichtig, dabei darauf zu achten, welche Datentypen im Dataframe enthalten sind. Denn es macht wenig Sinn z.B. das arithmetische Mittel von einer Datums- oder Textspalte zu berechnen. \n\nDie Funktion `.info` hilft uns, einen schnellen Überblick über die Datentypen im Dataframe zu erhalten. \n\nDas Ergebnis der Funktion ist eine Übersicht der Spalten (Column) und des jeweiligen Datentyps (Dtype). Darüber hinaus erhalten wir die Information, ob die jeweilige Spalte fehlende Informationen beinhaltet (sog. nans: not a number). Diese könnten wir dann bei Bedarf herausfiltern oder mit anderen Werten befüllen. Auf diese Technik gehen wir an dieser Stelle jedoch nicht ein. \n\nDie Funktion `.describe` liefert uns eine prägnante Übersicht an deskriptiven Statistiken. Da dies nur für numerische Spalten sinnvolle Ergebnisse liefert, werden alle anderen Spalten automatisch nicht berücksichtigt. So wird im untern Beispiel die Spalte `Datum` automatisch nicht berücksichtigt, da diese vom Datentyp `datetime` ist, d.h. ein Datumsformat hat. \n\nEine solche Übersicht ist hilfreich, um sich einen schnellen Überblick zur Verteilung der Daten zu verschaffen. Darüber hinaus können wir auch explizite Berechnungen für spezifische Spalten oder Zeilen durchführen.\n\nWir schauen uns dies am Beispiel des artithmetischen Mittels (engl. mean) und der Summe (engl. sum) an. \n\n**Aggregation je Spalte: Mean**\n\n**Aggregation je Zeile: Sum**\n\nPandas bietet über den Zusatz `axis=` an, auszuwählen, ob die Funktion spalten- oder zeilenweise ausgeführt werden soll. \n\n- `axis=0`: Funktion wird über die Spalte angewandt\n- `axis=1`: Funktion wird über die Zeile angewandt\n\n\n**Hinweis:** Wir können die obigen Berechnungen auch via Numpy durchführen, indem wir die jeweilige Numpy-Funktion nutzen und dieser den Dataframe oder die Series übergeben. \n\n## Zusammenfügen von Daten\n\nOft haben wir es mit verschiedenen Datenquellen zu tun, die in irgendeiner Art und Weise kombiniert werden sollen. Beispiele hierfür sind:\n\n- mehrere Datein, die Informationen je Monat beinhalten\n- mehrere Dateien, die Informationen je Produkt beinhalten\n- mehrere Dateine, die unterschiedliche Informationen beinhalten, sich jedoch inhaltlich ergänzen\n- ...\n\nWir können in Pandas Dataframes miteinander kombinieren. Wir schauen uns im Folgenden ein paar wichtige Ansätze an:\n\n### pd.concat\n\nMit der Funktion `pd.concat` können wir zwei oder mehr Datensätze zeilen- oder spaltenweise Verknüpfen. \n\nStellen wir uns vor, es gäbe zusätzlich zu unserem Beispieldatensatz noch einen weiteren Datensatz von gleichem Aufbau jedoch für einen unterschiedlichen Zeitraum. \n\n#### Zeilenweise Verknüpfung\n\nWir können diese Datensätze nun einfach zusammenfassen. Für diese Art der Daten macht es Sinn, die Daten zeilenweise zu aggregieren, d.h. wir fügen den zweiten Datensatz (df2) **unten** an den ersten Datensatz an.\n\nWir können dies mit der Funktion `pd.concat` erreichen. Da wir mit dieser Funktion sowohl spalten- als auch zeilenweise verknüpfen können, müssen wir über den Parameter `axis=0` angeben, dass wir eine zeilenweise Verknüpfung benötigen. \n\nSchaut man sich das Resultat an, dann stellt man fest, dass die Daten korrekt verknüpft wurden. Jedoch ist auch der jeweilige `Index` verknüpft worden. Im vorliegenden Falle führt dies zu Dopplungen im Index. Dies kann zu Problemen bei der weiteren Analyse führen und ist oft nicht sinnvoll - in unserem Falle z.B. sollten wir einen durchgehenden Index bevorzugen. Wir können dies auf zwei Wege erreichen:\n\n**Variante 1:** via `pd.concat` in dem wir den Parameter `ignore_index=True` übergeben\n\n**Variante 2:** via `reset_index`\n\nMit dem Zusatz `drop=True` geben wir an, dass wir den alten Index nicht mehr benötigen. Wird dieser Parameter mit `False` belegt (Hinweis: dies ist die Standardannahme), dann wird automatisch eine neue Spalte erzeugt, die den alten Index beinhaltet.\n\n#### Spaltenweise Verknüpfung\n\nIn den o.g. Beispielen haben wir zwei Datensätze zeilenweise verknüpft. Stellen wir uns nun vor, wir haben einen weiteren Datensatz mit zusätzlichen Informationen. \n\nErstellen wir zunächst einen fiktiven zusätzlichen Datensatz, der Informationen über die jeweiligen Vertriebler sowie die fixen Kosten am jeweiligen Tag beinhaltet.\n\nDer oben dargestellte (fiktive und zufällig generierte) Datensatz beinhaltet zusätzliche Informationen je Tag. Wir wollen diese Daten nun **rechts** mit dem bestehenden Datensatz verknüpfen. Da wir mit dieser Funktion sowohl spalten- als auch zeilenweise verknüpfen können, müssen wir diesmal über den Parameter `axis=1` angeben, dass wir eine spaltenweise Verknüpfung benötigen.   \n\n#### pd.merge\n\nIn unseren bisherigen Beispielen haben wir zwei oder mehrere Datensätze ohne Logik miteinander verknüpft. Oft ist eine Verknüpfung von Datensätzen jedoch komplexer. Stellen Sie z.B. vor, dass jeder Vertriebler für ein bestimmtes Produkt zuständig ist.\n\n- Herr Müller: Proukte 1, 3 und 5\n- Frau Meier: Produkt 2\n- Herr Schmidt: Produkt 4\n\nDiese Information haben wir in Form eines Dataframes vorliegen.\n\nEine Verknüpfung mit dem untenstehenden Datensatz ist nun komplexer, da wir hier weder zeilen- noch spaltenweise verknüpfen können.\n\nVielmehr benötigen wir eine Verknüpfungslogik in der eine Spalte \"Vertriebler\" hinzugefügt wird, jedoch die Werte in Abhängigkeit der jeweiligen \"Produkt Nr.\" des ursprünglichen Datensatzes gewählt werden. \n\nBeispiel: immer wenn im Datensatz `df_combined` bei Spalte `Produkt Nr.` eine 3 steht, solle in der neuen Spalte `Vertriebler` der Wert `Herr Meier` stehen.\n\nEine solche Verknüpfung können wir über `pd.merge` erzielen. Die Umsetzung sieht wie folgt aus:\n\nWir haben der Funktion `pd.merge` die Information übergeben auf Basis welcher Spalte verknüpft werden soll. In unserem Falle ist die gemeinsame Spalte `Produkt Nr.`. \n\nDas Ergebnis ist korrekt, jedoch nun anders sortiert. Dies können wir über `.sort_values` korrigieren (wenn wir wollen). In diesem Fall bietet es sich dann auch an, den Index neu zu definieren, um diesen wieder fortlaufend zu haben:\n\n```{admonition} Infos Verknüpfungen\n:class: tip, dropdown\nEs gibt viele weitere Varianten der Verknüpfung, auf die wir an dieser Stelle nicht eingehen wollen. Typischerweise ergeben sich im Laufe einer Analyse bestimmte Fragestellungen zu geeigneten Verknüpfungen. Sie sollten sich dann die Funktionen [`pd.concat`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) und [`pd.merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) genauer anschauen. \n```\n\n## Gruppieren von Daten\n\nWir werden im folgenden Kapitel Daten analysieren und dafür eine Reihe von Funktionen kennenlernen. Eine der wichtigsten Methoden zur strukturierten Analyse von Daten stellen wir an dieser Stelle bereits vor: `.groupby`. \n\nWir haben bereits einfache Aggregationen kennengelernt und z.B. das arithmetische Mittel über Spalten berechnet. Oft möchten wir z.B. eine solche Aggregation jedoch nicht für den gesamten Datensatz berechnen, sondern in Abhängigkeit von bestimmten anderen Spalten. In unserem oben dargestellten Datensatz ist z.B. eine Interessante Frage, welcher Vertriebler den höchsten gesamten oder durchschnittlichen täglichen Umsatz gemacht hat. Wir wollen also z.B. summieren oder mitteln in Abhängigkeit von jeweiligen Vertriebler.\n\nDie Schritte einer solchen Analyse sind dann im Detail wie folgt:\n\n1. Trenne Datensatz nach Bedingung (in unserem Falle nach Vertriebler)\n2. Aggriegiere jeden einzelnen Datensatz\n3. Füge aggregierte Datensätze wieder zusammen\n\nLassen Sie uns zunächst den Datensatz aufbereiten und die Schritte dann mit den uns bekannten Mitteln einzeln durchlaufen:\n\n### Manuell\n\n**Schritt 1:** Trennung nach Vertriebler\n\n**Schritt 2:** berechne Gesamtumsatz\n\n**Schritt 3:** Zusammenfügen der Daten\n\n\n### via .groupby\n\nDer gesamte obige Code kann durch eine einfache Zeile ersetzt werden\n\nDas oben vorgestellt Vorgehen ist wesentlich einfacher und folgt immer der gleichen Logik. Via `.groupby` wird angegeben, nach welcher Spalte oder nach welchen Spalten der Datensatz aufgeteilt werden soll. Das Resultat ist ein neuer Datentyp `DataFrameGroupBy`. Auf diesen Datentyp können dann neue Funktionen angewendet werden, die dann automatisch für jede einzelne Gruppe berechnet werden. Mit diesem Muster können viele scheinbar komplexe Berechnungen sehr schnell und übersichtlich durchgeführt werden. \n\nHier ein weiteres Beispiel: wir berechnen den durchschnittlichen Umsatz je Produkt\n\n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"Intro_Pandas.html"},"language":{},"metadata":{"lang":"de","fig-responsive":true,"quarto-version":"1.2.269","theme":"cosmo","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}}}